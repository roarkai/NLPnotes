{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220abefe",
   "metadata": {},
   "source": [
    "<font color=blue>**证明：given a sufficiently long language sequence $ W = (w_1, w_2, \\dots, w_N) $, the entropy of W , denoted as H(W), is equal to:**\n",
    "$$\n",
    "H(W) = -\\frac{1}{N} \\log P(w_1, w_2, \\dots, w_N)\n",
    "$$\n",
    "</font>\n",
    "<font color=brown>ref: chatGPT</font>\n",
    "### Step 1: Entropy Definition for a Sequence\n",
    "\n",
    "Consider a sequence of random variables $\\( W = (w_1, w_2, \\dots, w_N) \\)$, where each \\( w_t \\) is a word (or token) in the sequence.\n",
    "\n",
    "The **joint entropy** of the entire sequence \\( W \\) is defined as the entropy of the joint probability distribution $\\( P(w_1, w_2, \\dots, w_N) \\)$, which measures the uncertainty of the entire sequence. The entropy \\( H(W) \\) of the sequence \\( W \\) is given by:\n",
    "\n",
    "$$\n",
    "H(W) = - \\sum_{w_1, w_2, \\dots, w_N} P(w_1, w_2, \\dots, w_N) \\log P(w_1, w_2, \\dots, w_N)\n",
    "$$\n",
    "\n",
    "However, this expression gives the total entropy of the sequence, not the entropy **per word**. We are interested in the **average entropy per word**, which is the total entropy of the sequence divided by its length \\( N \\).\n",
    "\n",
    "### Step 2: Entropy per Word\n",
    "\n",
    "The **average entropy per word** \\( H(W) \\) is the joint entropy of the sequence divided by the length of the sequence \\( N \\):\n",
    "\n",
    "$$\n",
    "H(W) = \\frac{1}{N} \\cdot H(w_1, w_2, \\dots, w_N)\n",
    "$$\n",
    "\n",
    "Substituting the joint entropy into this expression:\n",
    "\n",
    "$$\n",
    "H(W) = -\\frac{1}{N} \\sum_{w_1, w_2, \\dots, w_N} P(w_1, w_2, \\dots, w_N) \\log P(w_1, w_2, \\dots, w_N)\n",
    "$$\n",
    "\n",
    "This is the formal expression for the **average entropy per word** in a sequence of length \\( N \\).\n",
    "\n",
    "### Step 3: Estimating Entropy from a Single Sequence\n",
    "\n",
    "Now, we want to estimate the entropy given a specific observed sequence $\\( W = (w_1, w_2, \\dots, w_N) \\)$. For a **specific observed sequence** \\( W \\), the probability of that sequence is $\\( P(w_1, w_2, \\dots, w_N) \\)$.\n",
    "\n",
    "Thus, for a single realization of the sequence \\( W \\), we can approximate the entropy by the negative log probability of the sequence, divided by the length \\( N \\). This gives:\n",
    "\n",
    "$$\n",
    "H(W) = - \\frac{1}{N} \\log P(w_1, w_2, \\dots, w_N)\n",
    "$$\n",
    "\n",
    "This expression represents the **empirical entropy per word** for the specific observed sequence. It estimates the average uncertainty per word in the sequence, based on the joint probability of the entire sequence.\n",
    "\n",
    "### Step 4: Intuition Behind the Expression\n",
    "\n",
    "The expression $\\( - \\frac{1}{N} \\log P(w_1, w_2, \\dots, w_N) \\)$ is an empirical estimate of the entropy of the sequence. The longer the sequence \\( N \\), the more accurately this expression estimates the true entropy per word. \n",
    "\n",
    "In a sufficiently long sequence:\n",
    "- The joint probability $\\( P(w_1, w_2, \\dots, w_N) \\)$ captures all the dependencies between the words in the sequence.\n",
    "- The quantity $\\( - \\log P(w_1, w_2, \\dots, w_N) \\)$ measures the **information content** or **surprise** of observing the specific sequence \\( W \\).\n",
    "- Dividing by \\( N \\) gives the **average information content per word**, which is the entropy per word for the sequence.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We have shown that for a sufficiently long sequence \\( W \\) of length \\( N \\), the **empirical entropy per word** can be estimated as:\n",
    "\n",
    "$$\n",
    "H(W) = -\\frac{1}{N} \\log P(w_1, w_2, \\dots, w_N)\n",
    "$$\n",
    "\n",
    "This expression provides an estimate of the entropy per word based on the joint probability of the entire sequence. As the sequence length \\( N \\) increases, this estimate becomes more accurate, reflecting the true average uncertainty or information content per word in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b7f129",
   "metadata": {},
   "source": [
    "### Step 3 Explanation: Approximating the Entropy for a Single Sequence \\( W \\)\n",
    "\n",
    "In this step, we explain why, for a **single realization** of a sequence $\\( W = (w_1, w_2, \\dots, w_N) \\)$, we can **approximate the entropy** by the **negative log probability** of that specific observed sequence, divided by its length \\( N \\).\n",
    "\n",
    "\n",
    "### 1. **Entropy as an Expected Value**\n",
    "\n",
    "Entropy \\( H(W) \\) is typically defined as the **expected** information content (or uncertainty) across all possible sequences of length \\( N \\). This means we compute entropy by averaging over all possible sequences weighted by their probabilities:\n",
    "\n",
    "$$\n",
    "H(W) = - \\sum_{w_1, w_2, \\dots, w_N} P(w_1, w_2, \\dots, w_N) \\log P(w_1, w_2, \\dots, w_N)\n",
    "$$\n",
    "\n",
    "However, we are interested in estimating the **empirical entropy** from a single observed sequence $\\( W = (w_1, w_2, \\dots, w_N) \\)$. In other words, given that we observe a specific sequence \\( W \\), how can we approximate its entropy?\n",
    "\n",
    "### 2. **Information Content of a Single Sequence**\n",
    "\n",
    "For a single sequence \\( W = (w_1, w_2, \\dots, w_N) \\), the **information content** (or \"surprise\") associated with observing this specific sequence is measured by the **negative logarithm of its probability**. This is based on the idea that **rare events** (those with low probability) contain more information, while **common events** (those with high probability) contain less information.\n",
    "\n",
    "The **information content** of the specific sequence \\( W \\) is:\n",
    "\n",
    "$$\n",
    "I(W) = - \\log P(w_1, w_2, \\dots, w_N)\n",
    "$$\n",
    "\n",
    "This tells us how much information is conveyed by observing this particular sequence \\( W \\).\n",
    "\n",
    "### 3. **Average Information per Word**\n",
    "\n",
    "Entropy is a measure of the **average information content per word** in a sequence. Since we are working with a single realization of the sequence, we can approximate the **average entropy per word** by dividing the total information content of the sequence by its length \\( N \\).\n",
    "\n",
    "So, the **average information content per word** is:\n",
    "$$\n",
    "H(W) \\approx \\frac{I(W)}{N} = - \\frac{1}{N} \\log P(w_1, w_2, \\dots, w_N)\n",
    "$$\n",
    "\n",
    "This quantity gives an estimate of the entropy per word for this specific sequence. It assumes that the observed sequence is typical or representative of the underlying distribution, and thus provides an approximation of the **true entropy**.\n",
    "\n",
    "### 4. **Justification: Law of Large Numbers and Typicality**\n",
    "\n",
    "The reason this approximation works well for a sufficiently long sequence \\( W \\) is based on concepts from information theory, particularly the **law of large numbers** and the concept of **typical sequences**.\n",
    "\n",
    "- **Law of Large Numbers**: As the sequence length \\( N \\) increases, the empirical distribution of the sequence tends to converge to the true distribution. This means that the observed sequence becomes more representative of the underlying probability distribution \\( P(x) \\).\n",
    "  \n",
    "- **Typicality**: In information theory, most sequences drawn from a probability distribution are **typical**. A typical sequence has a probability close to $\\( 2^{-N H(W)} \\)$, where \\( H(W) \\) is the true entropy of the distribution. In this case, the probability of the sequence $\\( P(w_1, w_2, \\dots, w_N) \\)$ is approximately related to the entropy as:\n",
    "\n",
    " $$\n",
    "  P(w_1, w_2, \\dots, w_N) \\approx 2^{-N H(W)}\n",
    "$$\n",
    "\n",
    "  Taking the logarithm of both sides gives:\n",
    "\n",
    "  $$\n",
    "  \\log P(w_1, w_2, \\dots, w_N) \\approx -N H(W)\n",
    "  $$\n",
    "\n",
    "  Dividing by \\( N \\), we get:\n",
    "\n",
    "  $$\n",
    "  -\\frac{1}{N} \\log P(w_1, w_2, \\dots, w_N) \\approx H(W)\n",
    "  $$\n",
    "\n",
    "Thus, for a sufficiently long sequence \\( W \\), the **negative log probability of the sequence divided by \\( N \\)** closely approximates the true entropy per word.\n",
    "\n",
    "### 5. **Interpretation**\n",
    "\n",
    "In essence, for a long sequence \\( W \\), the probability $\\( P(w_1, w_2, \\dots, w_N) \\)$ gives us an empirical measure of how \"typical\" or \"atypical\" the sequence is. Since entropy is fundamentally a measure of uncertainty or unpredictability, dividing the **total information content** of the sequence by \\( N \\) gives us an estimate of the **average unpredictability per word**.\n",
    "\n",
    "Thus, the expression $\\( - \\frac{1}{N} \\log P(w_1, w_2, \\dots, w_N) \\)$ provides a practical approximation for the entropy per word for a single observed sequence.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In summary, the reason we can approximate the entropy by $\\( - \\frac{1}{N} \\log P(w_1, w_2, \\dots, w_N) \\)$ for a single observed sequence \\( W \\) is because this quantity measures the **average information content per word** for that specific sequence. For sufficiently long sequences, this approximation becomes increasingly accurate, as the empirical probability of the sequence converges to the true underlying distribution, and the sequence becomes typical of the language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
