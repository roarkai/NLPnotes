{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2d5716",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "- ref: chapter3 of SLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34e866b",
   "metadata": {},
   "source": [
    "## I. Language Entropy\n",
    "### I.1 几种不同的Language Entropy定义\n",
    "#### I.1.1 word entropy\n",
    "1. 定义：在给定language中，假设word的生成函数是$P(x|context)$，则：\n",
    "   $$\\begin{align}\n",
    "H(x|context) & = \\sum_{c\\in L}P(context=c)H(x|context=c) \\\\\n",
    "& = \\sum_{c\\in L}P(context=c)\\left [\\sum_{x\\in L} -P(x|context=c)logP(x|context=c)\\right] \\\\\n",
    "& = -\\sum_{c\\in L}\\sum_{x\\in L}P(context=c)P(x|context=c)logP(x|context=c)\\\\\n",
    "& = -\\sum_{context\\in L}\\sum_{x\\in L}P(x,context)logP(x|context)\\\\\n",
    "& = -\\sum_{(x,context)\\in L}P(x,context)logP(x|context)\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11eebbc",
   "metadata": {},
   "source": [
    "#### I.1.2 per word entropy: sequence rate\n",
    "1. 符号\n",
    "   - n个word构成的sequence记为$X=x_1,x_2,...,x_n$\n",
    "   - sequence可以取language L中的任意长为N的sequence，记为$X\\in L$\n",
    "2. 定义：在给定language中，长为n的word sequence的熵为：\n",
    "$$\\begin{align}\n",
    "H(x_1,x_2,...,x_n) & = -\\sum_{X\\in L}P(X)logP(X)\\\\\n",
    "& = -\\sum_{X\\in L}P(x_1,x_2,...,x_n)logP(x_1,x_2,...,x_n)\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4ecec",
   "metadata": {},
   "source": [
    "3. 性质：sequence entropy是sequence中所有word entropy之和。\n",
    "     $$H(x_1,x_2,...,x_n)= H(x_1)+H(x_2|x_1)+...+H(x_n|x_1,...,x_{n-1})$$\n",
    "   - <font color=grey>证明：\n",
    "     $$\\begin{align}\n",
    "H(x_1,x_2,...,x_n) & = -\\sum_{X\\in L}P(x_1,x_2,...,x_n)\\sum_{i=1}^nlogP(x_i|x_1, ..., x_{i-1})\\\\\n",
    "& = -\\sum_{i=1}^n\\left[\\sum_{X\\in L}P(x_1,x_2,...,x_{\\color{red} n} )logP(x_i|x_1, ..., x_{i-1})\\right]\\\\\n",
    "& = -\\sum_{i=1}^n\\left[\\sum_{X\\in L}P(x_1,x_2,...,x_{\\color{red} i} )logP(x_i|x_1, ..., x_{i-1})\\right]\\\\\n",
    "& = H(x_1)+H(x_2|x_1)+...+H(x_n|x_1,...,x_{n-1})\\\\\n",
    "\\end{align}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64397230",
   "metadata": {},
   "source": [
    "4. **sequence entropy rate**: average entropy per word of a sequence\n",
    "   - 为了抵消sequence长度对熵的影响，定义<font color=blue>**Entropy rate**</font>:\n",
    "$$\\begin{align}\n",
    "H(X)& =\\frac{1}{n} H(x_1,x_2,...,x_n)\\\\\n",
    "& = -\\frac{1}{n}\\sum_{X\\in L}P(x_1,x_2,...,x_n)logP(x_1,x_2,...,x_n)\\end{align}$$\n",
    "   - entropy rate是average entropy per word\n",
    "   $$\\begin{align}\n",
    "H(X) & = \\frac{1}{n}H(x_1,x_2,...,x_n)\\\\\n",
    " & =\\frac{1}{n}[H(x_1)+H(x_2|x_1)+...+H(x_n|x_1,...,x_{n-1})]\\\\\n",
    "\\end{align}$$\n",
    "   - <font color=red>**注意符号：当X表示整个sequence的时候，H(X)是average entropy per word**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec9ebc",
   "metadata": {},
   "source": [
    "#### I.1.3 language entropy\n",
    "   - 将语言看做可以用来生成infinite sequence的随机过程L，定义Language entropy：\n",
    "$$\\begin{align}\n",
    "H(L) & = -\\lim_{n \\to \\infty }\\frac{1}{n}H(x_1, ..., x_n) \\\\\n",
    "& = -\\lim_{n \\to \\infty }\\frac{1}{n}\\sum_{X\\in L}P(x_1,...,x_n)logP(x_1,...,x_n)\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f581cc0",
   "metadata": {},
   "source": [
    "### I.2 sequence entropy的性质\n",
    "#### I.2.1 Language entropy: Shannon-McMillan-Breiman theorem\n",
    "1. 定理：如果language（对应的随机过程）是stationary且ergodic的，则当sequence长度$N\\rightarrow \\infty$时：$$\\begin{align}\n",
    "H(L) = \\lim_{n \\to \\infty }H(X) & = -\\lim_{n \\to \\infty }\\frac{1}{n}\\sum_{X\\in L}P(x_1,...,x_n)logP(x_1,...,x_n)\\\\\n",
    "& = -\\lim_{n \\to \\infty }\\frac{1}{n}logP(x_1,...,x_n)\n",
    "\\end{align}$$\n",
    "2. 理解：\n",
    "   - <font color=norange>**ergodic**</font>指：在一个随机过程中，单一样本函数(sample function)包括了整个随机过程的统计特征。它保证了如果有一个long sufficient sequence，那么该sequence的均值会converge to the expected value over all possible sequences.\n",
    "     $$\\lim_{n\\rightarrow \\infty}E(x_i|context)=E(x|context),x_i\\in sample sequence$$\n",
    "   - <font color=norange>**stationary**</font>指：由L生成的word sequence的概率分布不随时间变化。假设Language对应的生成模型是$P(x_i|context_i)$，stationary意味着每个word都是由相同的分布来生成的。\n",
    "     $$P(x_i|context)=P(x_j|context)$$\n",
    "   - <font color=blue>**intuition**</font>：如果language符合这两个性质，那么一个足够长的long sequence中会包含非常多其他短的sequence，并且这些短的sequence会按照他们在语言中的概率分布重复出现在这个long sequence中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9ca6c",
   "metadata": {},
   "source": [
    "3. <font color=green>该定理的用途：将语言的熵转化为长度无限的单个sequence的统计特征。此时可以用样本sequence的统计特征来近似估计语言的熵。</font>\n",
    "   - 如果language对应的随机过程是ergodic的，那么sufficient long sequence能够包含随机过程的统计特征，此时：\n",
    "$$\\begin{align}\n",
    "& -\\lim_{n \\to \\infty }\\frac{1}{n}logP(x_1,...,x_n)  =-\\frac{1}{n}logP(x_1,...,x_n) \\\\\n",
    "& \\therefore H(L) = -\\frac{1}{n}logP(x_1,...,x_n)\n",
    "\\end{align}$$\n",
    "4. 注：<font color=red>Shannon-McMillan-Breiman theorem不同于Monte Carol sampling</font>\n",
    "   - 如果用MC sampling，则表达式中的期望值应该用样本均值表示如下：$$\\begin{align}\n",
    " H(X) & = -\\frac{1}{N}\\sum_{X\\in L} P(x_1, ..., x_N)logP(x_1, ..., x_N)\\\\\n",
    "& = -\\frac{1}{N}\\left[\\frac{1}{M}\\sum_{k=1}^{M}logP(x_{k1}, ..., x_{kN})\\right]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80d613",
   "metadata": {},
   "source": [
    "#### I.2.2 sequence entropy\n",
    "- <font color=red>[提供了对上一条的证明]</font>\n",
    "- 如果一个language对应的随机过程是ergodic且stationary的，那么用该语言随机生成一个sequence X的per word entropy可以用该语言所生成的一个sufficiently long sequence 样本{x_1, x_2, ..., x_N}的自信息来估计。其值等于随机过程对应的概率分布函数的熵。\n",
    "$$\\begin{align}\n",
    "\\lim_{N \\to \\infty}H(X) & =-\\lim_{N \\to \\infty}\\frac{1}{N}logP(x_1, ..., x_N) \\\\\n",
    "& \\approx -\\frac{1}{N}logP(x_1, ..., x_N) \n",
    "\\end{align}$$\n",
    "  - <font color=red>注：这里$X\\ne {x_1, x_2, ..., x_N}$。左边H(X)是随机变量X的分布P(X)对应的熵，X是随机变量而不是具体的某一个sequence，且其长度不一定是N。右边则是某个特定的长为N（sufficiently long）的样本序列(event)的自信息。</font>\n",
    "  - <font color=green>[证明见附录1，该结论证明是rk参考claude完成]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f550f27",
   "metadata": {},
   "source": [
    "## II. Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504af660",
   "metadata": {},
   "source": [
    "### II.1 language entropy的实践含义\n",
    "#### II.1.1 H(x)是随机变量x的平均最优码长\n",
    "1. 熵是随机变量自信息的期望值，是用bit为单位计量的随机变量所携带的自信息的平均大小。\n",
    "   $$H(x)=-\\sum_{x\\in \\mathcal{X}}P(x)log_2P(x)$$\n",
    "2. **事件x**的自信息是事件x的最优编码长度。如果对随机变量取值空间中的每个event都用最优编码，那么该随机变量(用bit为单位计量)的平均码长等于该随机变量的熵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ee78a",
   "metadata": {},
   "source": [
    "#### II.1.2 effective branching factor = $2^{H(x)}$\n",
    "- 离散随机变量x的事件空间为$\\mathcal{X}$，事件空间中event数$|\\mathcal{X}|=k$.（简化假设$k=2^d$，d是整数，因此没有round up to integer的问题）。当x是等概率分布时，熵最大，值为$H(x)=-log\\frac{1}{k}=logk=d$。\n",
    "   - 可见，H(x)=d对应着：有$2^d$种取值的等概率分布。将$2^d$称为<font color=norange>分布的**effective number of equally probable choices**</font>，也称为<font color=norange>分布的**effective braching factor**</font>\n",
    "- 理解：当分布的熵为d时，其信息量等价于一个有$2^d$种等概率取值的分布所含信息量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3efe96c",
   "metadata": {},
   "source": [
    "### II.2. 随机变量分布的Perplexity\n",
    "#### II.2.1 x是单个随机变量\n",
    "1. 定义：$$Perplexity(x)=2^{H(x)}$$\n",
    "2. 理解：\n",
    "   - 当x是一个离散随机变量时，它的Perplexity的值就是其分布的effective number of equally probable choices。也就是<font color=norange>分布的**effective braching factor**</font>\n",
    "   - <font color=green>Perplexity一般用于measure the difficulty of a prediction problem。</font>\n",
    "3. 在语言环境中的含义\n",
    "   - <font color=red>注：这里用的是information theory中Perplexity的含义，实际上在machine learning中，Perplexity定义的对象不是分布，而是sample</font>\n",
    "   - 在语言中，$P(x|context)$的熵为$H(x|context)$，对应的$Perplexity(x|context)=2^{H(x|context)}$，其含义为基于context来生成next word时，有$2^{H(x)}$个等概率的next word。<font color=deeppink>**$2^{H(x)}$ is the effective number of choices that the model faces when predicting the next word.**</font>\n",
    "#### II.2.2 X是word sequence\n",
    "1. 定义：\n",
    "$$\\begin{align}\n",
    "Perplexity(X)& =2^{ H(X)}\\\\\n",
    " & = 2^{-\\frac{1}{N}logP(x_{1}, x_2, ..., x_N)}\\\\\n",
    "& = P(x_1, x_2, ..., x_N)^{-\\frac{1}{N}}\\\\\n",
    "& = \\left( \\prod_{i=1}^N P(x_i|x_1, ..., x_{i-1})\\right )^{-\\frac{1}{N}}\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e3713",
   "metadata": {},
   "source": [
    "2. 在语言环境中的含义：\n",
    "   - 长为N的word sequence X，其分布P(X)的熵为H(X)。则X的Perplexity有：\n",
    "     $$\\begin{align}\n",
    "Perplexity(X) & = 2^{H(X)} \\\\\n",
    "& = 2^{\\frac{1}{N}H(x_1, x_2, ..., x_N)} \\\\\n",
    "& = 2^{\\frac{1}{N}[H(x_1)+H(x_2|x_1)+ ...+H(x_N|x_1,...,x_{N-1})]} \\\\\n",
    "& = \\left (2^{[H(x_1)+H(x_2|x_1)+ ...+H(x_N|x_1,...,x_{N-1})]}\\right)^{\\frac{1}{N}} \\\\\n",
    "& = \\left (2^{H(x_1)}*2^{H(x_2|x_1)}*...*2^{H(x_N|x_1,...,x_{N-1})}\\right)^{\\frac{1}{N}} \\\\\n",
    "& = \\left [ ppl(x_1)* ppl(x_2)* ... * ppl(x_N)\\right ]^{\\frac{1}{N}} \\\\\n",
    "\\end{align}$$\n",
    "       - 可见，<font color=blue>Perplexity of a sequence等于单个word的Perplexity的几何平均。</font>\n",
    "       - 从Perplexity的实践含义来看，单个word的Perplexity表示the effective number of choices of one word，他们的几何平均就是**the average of the effective number of choices of the words in the sequence**.也称为<font color=norange>**average effective braching factor of the words in the sequence**</font>.\n",
    "       - <font color=green>**Perplexxity of word sequence**: the effective number of choices a model faces when predicting the next word, averaged over all positions in a sequence.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d9ddd",
   "metadata": {},
   "source": [
    "#### II.2.3 例子\n",
    "- 引例1：<font color=orange>假设有一种由A/B/C/D四个值构建的语言。在该语言生成的序列中，$P(x_i)$独立同分布，且$P(x_i=A)=P(x_i=B)=P(x_i=C)=P(x_i=D)=\\frac{1}{4}$。那么该语言的branching factor就是4。</font>注：4只是branching factor，不是average weighted branching factor。\n",
    "- 引例1续：<font color=orange>此时该语言的PPL=4。</font>\n",
    "  - 证明：\n",
    "      $$\\begin{align}\n",
    "H(x_i) & = -4*(\\frac{1}{4}*log\\frac{1}{4})=2 \\\\\n",
    "Perplexity(X) & = \\left [ ppl(x_1)* ppl(x_2)* ... * ppl(x_N)\\right ]^{\\frac{1}{N}} \\\\\n",
    "& = \\left(4^N \\right)^{\\frac{1}{N}} \\\\\n",
    "& = 4\n",
    "\\end{align}$$\n",
    "- 引例2：<font color=orange>如果trainning set和test set中每个digit出现的概率改为$P(x_i=A)=\\frac{1}{2},P(x_i=B)=\\frac{1}{4},P(x_i=C)=P(x_i=D)=\\frac{1}{8}$。则此时可以预计perplexity比上面等概率的时候要低。因为大多数时候出现的都是A，确定性高.</font>\n",
    "  - 分析PPL：\n",
    "      $$\\begin{align}\n",
    "H(x_i) & = -\\left(\\frac{1}{2}*log\\frac{1}{2}+ \\frac{1}{4}*log\\frac{1}{4}+ \\frac{1}{8}*log\\frac{1}{8} + \\frac{1}{8}*log\\frac{1}{8}\\right )=\\frac{7}{4}  \\\\\n",
    "Perplexity(X) & = \\left [ ppl(x_1)* ppl(x_2)* ... * ppl(x_N)\\right ]^{\\frac{1}{N}} \\\\\n",
    "& = \\left(2^{\\frac{7}{4}N} \\right)^{\\frac{1}{N}} \\\\\n",
    "& = 2^{\\frac{7}{4}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1159643",
   "metadata": {},
   "source": [
    "### II.3 Probability model的Perplexity\n",
    "[详见NLPnotes的evaluation章节]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806e2d6",
   "metadata": {},
   "source": [
    "### 附录\n",
    "#### 1. 证明sequence entropy的性质\n",
    "- <font color=brown>ref: claude</font>\n",
    "\n",
    "- 如果一个language对应的随机过程是ergodic且stationary的，那么用该语言随机生成一个sequence X的per word entropy可以用该语言所生成的一个sufficiently long sequence 样本{x_1, x_2, ..., x_N}来估计。\n",
    "$$\\begin{align}\n",
    "\\lim_{N \\to \\infty}H(X) & =-\\lim_{N \\to \\infty}\\frac{1}{N}logP(x_1, ..., x_N) \\\\\n",
    "& \\approx -\\frac{1}{N}logP(x_1, ..., x_N) \n",
    "\\end{align}$$\n",
    "  - <font color=red>注：这里$X\\ne {x_1, x_2, ..., x_N}$。左边H(X)是随机变量X的分布P(X)对应的熵，X是随机变量而不是具体的某一个sequence，且其长度不一定是N。右边则是某个特定的长为N的样本序列(event)的自信息。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53308f",
   "metadata": {},
   "source": [
    "- 推导：\n",
    "  1. 根据sequence entropy的定义：$$\\begin{align}H(X) & = \\frac{1}{n}H(x_1,x_2,...,x_n)\\\\\n",
    "& = -\\frac{1}{n}\\sum_{X\\in L}P(x_1,x_2,...,x_n)logP(x_1,x_2,...,x_n)\\\\\n",
    "& = -\\frac{1}{n}\\sum_{X\\in L}P(x_1,x_2,...,x_n)\\sum_{i=1}^NlogP(x_i|x_1, ..., x_{i-1})\\\\\n",
    "& = -\\frac{1}{n}\\sum_{i=1}^N\\left[\\sum_{X\\in L}P(x_1,x_2,...,x_{\\color{red} n} )logP(x_i|x_1, ..., x_{i-1})\\right]\\\\\n",
    "& = -\\frac{1}{n}\\sum_{i=1}^N\\left[\\sum_{X\\in L}P(x_1,x_2,...,x_{\\color{red} i} )logP(x_i|x_1, ..., x_{i-1})\\right]\\\\\n",
    " \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16cb6e9",
   "metadata": {},
   "source": [
    "2. 如果<font color=red>language对应的随机过程</font>是stationary且ergodic的，那么当$n\\to \\infty$时有：$$\\begin{align}\n",
    "P(x_i|x_1,...,x_{i-1}) & =P(x_j|x_1,...,x_{j-1}),\\forall j\\ne i\n",
    "\\end{align}$$\n",
    "   - 此时有：$$\\lim_{n\\to \\infty}H(X) = E[-logP(x|context)]...[1]$$\n",
    "     - 注：这里要用极限形式，因为ergodic性质是以$n\\to \\infty$为前提的。\n",
    "   - 证明：$$\\begin{align}\n",
    "\\lim_{n\\to \\infty}H(X) & = \\lim_{n\\to \\infty}-\\frac{1}{n}\\sum_{i=1}^n\\left[\\sum_{X\\in L}P(x_1,x_2,...,x_i )logP(x_i|x_1, ..., x_{i-1})\\right]\\\\\n",
    "& = \\lim_{n\\to \\infty}-\\frac{1}{n}*n\\left[\\sum_{X\\in L}P(x_1,x_2,...,x_i )logP(x_i|x_1, ..., x_{i-1})\\right]\\color{Green} {\\because stationary }\\\\\n",
    "& = -\\sum_{X\\in L}P(x_1,x_2,...,x_i )logP(x_i|x_1, ..., x_{i-1})\\\\\n",
    "& = E[-logP(x_i|x_1, ..., x_{i-1})]\\\\\n",
    "& = E[-logP(x_i|context)]\\\\\n",
    "& = E[-logP(x|context)] \\color{Green} {\\because ergodic}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8ff44",
   "metadata": {},
   "source": [
    "3. 如果生成<font color=red>特定的长为N(sufficiently long)的sequence ${x_1, ..., x_N}$</font>的随机过程是stationary且ergodic的，可以证明：$$-\\lim_{N \\to \\infty}\\frac{1}{N}logP(x_1, ..., x_N) = E[-logP(x|context)]...[2]$$\n",
    "   - 证明：$$\\begin{align}\n",
    "-\\lim_{N \\to \\infty}\\frac{1}{N}logP(x_1, ..., x_N) & = -\\lim_{N \\to \\infty}\\frac{1}{N}\\sum_{i=1}^NlogP(x_i|x_1, ..., x_{i-1}) \\\\\n",
    "& = -\\lim_{N \\to \\infty}\\frac{1}{N}\\sum_{k}m_klogP(x_k|context_k)\\\\\n",
    "& = -\\lim_{N \\to \\infty}\\sum_{k}\\frac{m_k}{N}logP(x_k|context_k)   \\end{align}$$k是sequence中不同$word|context$组合的index,有的$word|context$会重复出现，因此用$m_k$记录$x_k|context_k$出现的次数。\n",
    "   - 根据大数定律，有$N\\rightarrow \\infty$时，$\\frac{m_k}{N}=P(x_k|context_k)$\n",
    "   - 因为sequence X是sufficiently long的，且生成它的stochastic process是ergodic的，因此N中不同的$x_k|context_k$包含了所有language中可能的$word|context$组合。\n",
    "\n",
    "   - 代入上式：$$\\begin{align}\n",
    "& -\\lim_{N \\to \\infty}\\frac{1}{N}logP(x_1, ..., x_N) \\\\\n",
    "& = -\\lim_{N \\to \\infty}\\sum_{k}\\frac{m_k}{N}logP(x_k|context_k)   \\\\\n",
    "& = -\\lim_{N \\to \\infty}\\sum_{k}P(x_k|context_k)logP(x_k|context_k) \\color{Green} {\\because 大数定律}   \\\\\n",
    "& = E[-logP(x_i|context)] \\color{Green} {\\because ergodic} \\\\\n",
    "& = E[-logP(x|context)]  \\color{Green} {\\because stationary} \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419473f",
   "metadata": {},
   "source": [
    "4. 根据[1]和[2]有：\n",
    "$$\\begin{align}\n",
    "\\lim_{N \\to \\infty}H(X) & =-\\lim_{N \\to \\infty}\\frac{1}{N}logP(x_1, ..., x_N) \\\\\n",
    "& \\approx -\\frac{1}{N}logP(x_1, ..., x_N) \n",
    "\\end{align}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
