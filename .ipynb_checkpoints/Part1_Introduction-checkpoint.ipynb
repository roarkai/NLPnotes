{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2d5716",
   "metadata": {},
   "source": [
    "# Part I: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd769f1e",
   "metadata": {},
   "source": [
    "## 1. 用Generative和Discriminative Model处理文本分类问题\n",
    "**文本分类问题：** input X是sentence，label y是该sentence的类型标签。求$\\hat{y}=\\underset{y}{argmax}P(y|X)$ \\\n",
    "**关键：** 找到$P(y|X)$的分布形态 \\\n",
    "**方法：** \\\n",
    "· 概率方法：生成模型和判别模型（本节主要关注这两种方法的差异） \\\n",
    "· 非概率方法：SVM，Perceptron，NN不用softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f0cf6",
   "metadata": {},
   "source": [
    "### 1.1 生成模型和判别模型\n",
    "**<font color=orange>Generative model</font>**\n",
    "1. 模型训练时：用贝叶斯分析方式对$𝑃(𝑦|𝑋)$做分解，转化为对先验分布$P(y_i;φ)$和似然$P(X|y_i;θ)$建模，用MLE或者CE做目标函数，求解参数θ。\\\n",
    "贝叶斯分析：\\\n",
    "$P(y_i|X) = \\frac{P(X|y_i)*P(y_i)}{P(X)} = \\frac{P(X|y_i)*P(y_i)}{Σ_{y_j\\in{Y}} P(X|y_j)*P(y_j)}$ \\\n",
    "求解MLE：\\\n",
    "$\\hat{θ}=\\underset{θ,φ}{argmax}\\ L(θ,φ) = \\underset{θ,φ}{argmax}\\ \\frac{P(X|y_i;θ)*P(y_i;φ)}{Σ_{y_j\\in{Y}} P(X|y_j;θ)*P(y_j;φ)}$\n",
    "2. 模型推理时：同样利用贝叶斯关系对目标函数做分析，求解优化函数。 \\\n",
    "利用贝叶斯分析： \\\n",
    "$\\hat{y}=\\underset{y}{argmax}P(y|X) = \\underset{y}{argmax}\\frac{P(X|y)*P(y)}{P(X)}$ \\\n",
    "<font color=red>由于y的取值与P(X)无关</font>，因此有：\\\n",
    "$\\hat{y}=\\underset{y}{argmax}\\ P(X|y;\\hat{θ})*P(y;\\hat{φ})=\\underset{y}{argmax}\\ P(X,y;\\hat{θ},\\hat{φ})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1565dcaf",
   "metadata": {},
   "source": [
    "**<font color=orange>Discriminative model</font>**\n",
    "1. 模型训练时：直接对$P(y|X)$的参数建模，用MLE或者CE做目标函数，求解参数θ:\n",
    "$\\hat{θ}=\\underset{θ}{argmax}\\ L(θ) = \\underset{θ}{argmax}\\ \\underset{i}{Σ}logP(y_{i}|X_{i};θ)$\n",
    "2. 模型推理时：直接用估计的条件概率分布求概率最大的类型： \\\n",
    "$\\hat{y}=\\underset{y}{argmax}P(y|X;\\hat{θ})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5405c0d",
   "metadata": {},
   "source": [
    "### 1.2 用简单的生成模型处理文本分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6531b",
   "metadata": {},
   "source": [
    "#### 1.2.1 用生成模型解决问题的基础：语言模型\n",
    "1. 生成模型在将后验概率转化为先验\\*似然之后，实际应用场景下，比如分类问题中，一般会直接用训练集统计先验概率$P(y_i)$。然后分别对各个不同分类的样本集单独训练$P(X|y_i)$。从单个分类数据集而言，可以简写为$P(X)$\n",
    "2. 在自然语言处理的语境中，X就是sentence。计算P(X)也就是计算sentence发生的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1b47d",
   "metadata": {},
   "source": [
    "<font color=blue>**语言模型：P(X)** </font>\n",
    "1. **定义：models that assign probabilities to sequences of words.**\n",
    "2. **Chain rule：**$P(X)= \\prod_{i=1}^{I}P(x_i|x_1, x_2, ···,x_{i-1}) $  ，I是sentence长度\n",
    "3. <font color=red>求解语言模型的关键是估计$P(x_i|x_1, x_2, ···,x_{i-1})$</font>\n",
    "4. <font color=green>**最简单的近似估计方法是n-gram models：**</font>\n",
    "$P(x_i|x_1, x_2, ···, x_{i-1}) \\approx  P(x_i|x_{i-n+1}, x_{i-n+2}, ···, x_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f0784",
   "metadata": {},
   "source": [
    "#### 1.2.2 最简单的语言模型：count-based unigram model\n",
    "**特点：**\n",
    "1. naive bayesian假设：$P(x_i|x_1, x_2, ···, x_{i-1})=P(x_i)$ \\\n",
    "此时，得到的语言模型是unigram model：$P(X)= \\prod_{i=1}^{I}P(x_i) $\n",
    "2. 模型训练过程中，使用的求解$P(x_i)$的方法：MLE\n",
    "3. 用MLE得到的结果是count-based estimate： \n",
    "$P_{MLE}(x_i)=\\frac{C_{train}(x_i)}{Σ_{\\tilde{x}}C_{train}(\\tilde{x})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78a53a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T16:25:10.129056Z",
     "start_time": "2023-11-01T16:25:10.124749Z"
    }
   },
   "source": [
    "#### 1.2.3 BOW(Bag-of-words)生成模型: 又称为Naive Bayes\n",
    "![BOW_generative_model](./pics/BOW_generative_model.png)\n",
    "图中$θ_y$是在trainning set中计算的count-based probability。 \\\n",
    "用公式表达为：$P(X,y)=P(y)* {\\textstyle \\prod_{i=1}^{|X|}}P(x_i|y)=\\frac{c(y)}{Σ_{y_j}c(y_j)}  {\\textstyle \\prod_{i=1}^{|X|}}\\frac{c(x_i, y)}{Σ_{x_k}c(x_k, y)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4d1260",
   "metadata": {},
   "source": [
    "#### 1.2.4 n-gram model\n",
    "**特点：**\n",
    "1. 假设：每个词出现的概率与前n-1个位置的词语有关。 \\\n",
    "$P(x_i|x_1, x_2, ···, x_{i-1})=P(x_i|x_{i-n+1}, x_{i-n+2}, ···, x_{i-1})$，简记为：$P(x_i|x_{i-n+1}:x_{i-1})$ \n",
    "\n",
    "2. 模型训练过程中，使用的求解$P(x_i|x_{i-n+1}:x_{i-1})$的方法：MLE\n",
    "3. 用MLE得到的结果是count-based estimate： \n",
    "$P_{MLE}(x_i|x_{i-n+1}:x_{i-1})=\\frac{C_{train}(x_{i-n+1}:x_i)}{Σ_{\\tilde{x}}C_{train}(x_{i-n+1}:x_{i-1},\\tilde{x})}=\n",
    "\\frac{C_{train}(x_{i-n+1}:x_i)}{C_{train}(x_{i-n+1}:x_{i-1})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a65d6",
   "metadata": {},
   "source": [
    "**count table：** \\\n",
    "例：取n=2时，用Berkeley Restaurant Project的数据(|V|=1446)给8个words统计count table\n",
    "<img src=\"./pics/BerkeleyRestaurantCountTable.png\" style=\"zoom:80%\">\n",
    "\n",
    "将count table中的count转化为probability：\n",
    "<img src=\"./pics/BerkeleyRestaurantProbTable.png\" style=\"zoom:80%\">\n",
    "\n",
    "这种count table normalize之后拿到的probability可以反映语料的信息： \\\n",
    "(1) 语言本身的语义信息，比如：动词后面通常是名词或形容词  \\\n",
    "(2) 语言内容中体现的文化信息，比如：上表中，大比例的食物是中餐而不是西餐，说明语料涉及的人们更喜欢中餐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18771ac8",
   "metadata": {},
   "source": [
    "#### 1.2.5 N-gram model训练结果中的问题\n",
    "1. **N-gram model训练结果有两个典型特征：** \\\n",
    "<font color=orange>(1) model的N越大，训练好的模型用sample method得到的句子的连贯性看起来会越好。</font>但这并不是模型生成语句的能力变强，而是model记忆了trainning set，这种记忆没有泛化能力。 \\\n",
    "· 在莎士比亚文本语料（88万个word）基础上训练unigram到four-gram model，然后在训练好的模型上用sample methods得到下图结果。\n",
    "<img src=\"./pics/N-gramOnShakespear.png\" style=\"zoom:100%\"> \\\n",
    "<font color=orange>(2) 训练得到的probability通常包含了训练语料中的具体信息。</font>这同样只是model记忆了trainning set的体现。一旦使用场景的语言特征和训练语料的差异较大，那么模型效果就会很差。\\\n",
    "· 比较下图中用WSJ语料（4000万个word）上训练的uni-gram到tri-gram model做sample的结果与莎士比亚sample结果可以看到两者的语言风格截然不同。\n",
    "<img src=\"./pics/N-gramWSJ.png\" style=\"zoom:100%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71768bc6",
   "metadata": {},
   "source": [
    "2. <font color=red>**造成上述现象的原因是N-gram model有Sparsity的问题。** 随着N和|V|的增加，训练结果中$P(x_i|x_{i-n+1}:x_{i-1})=0$的占比越来越高，n-gram probability matrix越来越sparse。</font> \\\n",
    "**分析：** \\\n",
    "<font color=blue>**N,|V|与probability matrice大小的关系：** $matrice中的elements数量=|V|^{N}$ </font> \\\n",
    "N=2，|V|=29k时，$|V|^2\\approx 8e8$ \\\n",
    "N=3，|V|=29k时，$|V|^2\\approx 2e13$ \\\n",
    "N=4，|V|=29k时，$|V|^2\\approx 7e17$ \\\n",
    "<font color=orange>一般的语料库很难达到让probability matrx变得dense所需的大小。</font>莎士比亚语料的词典长度|V|是29k，corpus长度大约9e5；WSJ语料|V|是20k，corpus长度大约4e7。而这些corpus去掉重复的N-gram后，覆盖matrix的比例更小。 \\\n",
    "在句子生成场景，这种sparsity意味着，如果用sample method来generate sentence，那么很多时候会抽到corpus中的原句。因为probability matrix中很多row里面只有1个element的概率非0,且为1。这就解释了模型能记忆原文的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c8f37",
   "metadata": {},
   "source": [
    "3. <font color=blue>**sparsity带来的问题：** </font> \\\n",
    "(1) 只有很少比例的n-gram组合的probability非零，使得model只是记忆了trainning set，这种记忆没有泛化能力。(如前所述)对这个问题的一个补救方式是，根据应用场景来选择与之match的语料库。当然，这不会提升模型本身的能力。但是是工程上好用的方式。 \\\n",
    "(2) 在用perplexity做模型评估的时候还会因为test set中有很多n-gram组合的probability在训练时赋值为0，导致perplexity很容易是0.  \\\n",
    "<font color=green>**所以更本质的解决问题的方式是：找到处理zero probabilities的方法。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671958e",
   "metadata": {},
   "source": [
    "4. **处理zero-probability：** \\\n",
    "zero-probabilty有两类，一类是unknown words，一类是出现在novel test set context的n-gram词组。各有不同的处理方式。\\\n",
    "(1) unknown words：n-gram中出现了词典中没有的词。\\\n",
    "(2) novel n-gram词组：词典中有n-gram中出现的单个word，但是他们的组合方式在trainning set中没有出现过。这种情况一般用smoothing，也称为discounting。思路是划分一部分概率给novel n-gram，使其最低值>0. \\\n",
    "还有一种实际更常用的方式是，不用words作为基本单位来训练模型，而是用characters或者sub-words。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cad96b",
   "metadata": {},
   "source": [
    "#### 1.2.6 处理unknown words\n",
    "1. 在词典中加上\\<UNK>，所有unknown words都当做\\<UNK>处理。\\\n",
    "方法1： 如果原本有一个vocabulary： \\\n",
    "一种最简单但不是非常合理的处理方式是，在模型训练时，trainning set中出现，但没有出现在vocabulary中的词都转化为\\<UNK>，并将\\<UNK>当做常规词一样处理。test的时候，也将所有不在vocabulary中的词转化为\\<UNK>再将\\<UNK>当做一个常规词处理。\n",
    "方法2： 如果原本没有一个vocabulary： \\\n",
    "用trainning set建一个implicit vocabulary。具体方法是：将trainning set中出现频率低于一定数量的词语都转化成\\<UNK>，其他words则每个都作为一个单独的word放入vocabulary。训练和测试则和方法1中相同。 \\\n",
    "2. <font color=blue>要注意的是，处理\\<UNK>的具体方式会影响perplexity(什么是perplexity详见后文)的结果。因此，要比较两个model之间的perplexity的时候，一定要确保他们用的vocabulary是一样的。</font>  \\\n",
    "eg: 将大量word都处理成\\<UNK>可以降低vocabulary的词汇量。模型可以通过选择一个很小的vocabulary来降低perplexity。这时候\\<UNK>被赋予的probability会很高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e5cad",
   "metadata": {},
   "source": [
    "#### 1.2.7 smoothing: 处理novel n-gram\n",
    "**问题：** test时有的n-gram组合在trainning set中没有出现过，但是词典中有n-gram中所包含的每一个word。由于在trainning data中没有，直接用count-based MLE会得到count=0，$P(x_{unk})=0$，导致test时出现：L(θ)=0\n",
    "\n",
    "**处理思路：** 需要一种分布，对所有可能的words所赋予的概率都大于0.\n",
    "1. 最常用的处理方法：character/subword-based model\n",
    "2. smoothing\n",
    "3. uniform distribution：基本不用 \\\n",
    "· 思想：在训练模型时，假设|V|固定，并让所有的unknown words的概率为$P_{unk}=\\frac{1}{N_{|V|}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f771bd",
   "metadata": {},
   "source": [
    "**第一种方法：Laplace smoothing(add-one smoothing)** \\\n",
    "· <font color=blue>思想：在训练模型时，给所有n-gram组合的count都加1，使得unknown n-grams的count=1。</font> \\\n",
    "因此：\\\n",
    "$adjusted\\ P(x_i)=\\frac{C_{train}(x_i)+1}{Σ_{\\tilde{x}}(C_{train}(\\tilde{x})+1)} = \\frac{C_{train}(x_i)+1}{Σ_{\\tilde{x}}C_{train}(\\tilde{x})+|V|}$ \\\n",
    "· 问题：\\\n",
    "i. unknown n-grams的数量会很大，会导致$|V| \\to \\infty$,使$adjusted\\ P(x_i)\\to 0$ \\\n",
    "ii. 如果unknown n-grams的数量大，也会过度扭曲原分布。\\\n",
    "<font color=orange>扭曲程度分析思路：以bi-gram为例</font> \\\n",
    "找一组新的n-gram counts$\\ C(x_{i-1}, x_i)^{'}$，在不改变$C(x_{i-1})$的条件下，满足$P_{MLE}^{'}(x_i|x_{i-1})=P_{add-1}(x_i|x_{i-1})$。此时，比较$C(x_{i-1}, x_i)^{'}$和$C(x_{i-1}, x_i)$的差异。\\\n",
    "$${\\begin{align} \n",
    "P_{MLE}^{'}(x_i|x_{i-1}) & =\\frac{C(x_{i-1}, x_i)^{'}}{C(x_{i-1})}=P_{add-1}(x_i|x_{i-1}) \\\\\n",
    "C(x_{i-1}, x_i)^{'}& = P_{add-1}(x_i|x_{i-1}) * C(x_{i-1}) \\\\\n",
    "& = \\frac{C(x_{i-1}, x_i)+1}{C(x_{i-1})+|V|} * C(x_{i-1}) \\\\\n",
    "& = \\frac{1}{1+\\frac{|V|}{C(x_{i-1})}} * [C(x_{i-1}, x_i) +1 ] \\\\\n",
    "& \\approx \\frac{1}{1+\\frac{|V|}{C(x_{i-1})}} * C(x_{i-1}, x_i)\n",
    "\\end{align}}$$\n",
    "<font color=red>此时，$C(x_{i-1}, x_i)^{'}$和$C(x_{i-1}, x_i)$的差异就体现了smoothing对语料的扭曲效果。从公式可见，差异由|V|决定。|V|越大，probability matrix被扭曲的程度越大。</font> \\\n",
    "<font color=green>**所以laplace smoothing一般只用在有零概率问题，但是不严重(|V|较小)的场景，比如unigram中，n-gram不用。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a78ef",
   "metadata": {},
   "source": [
    "**第二种方法：add-k smoothing** （略）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d615b4b",
   "metadata": {},
   "source": [
    "**第三种方法：backoff and interpolation** \\\n",
    "思路：当$P(x_i|x_{i-n+1}:x_{i-1})=0$时，用$P(x_i|x_{i-n+2}:x_{i-1})$来代替，也就是backoff to short sequence。 \\\n",
    "扩展到interpolation的思路：同时用unigram到n-gram的加权概率来做n-gram probability。\\\n",
    "eg：$P^{'}(x_i|x_{i-2},x_{i-1})=λ_1*P(x_i)+λ_2*P(x_i|x_{i-1})+(1-λ_1-λ_2)*P(x_i|x_{i-2},x_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff244db",
   "metadata": {},
   "source": [
    "### 1.3 用判别模型解决文本分类问题的方法\n",
    "判别模型的训练思路：$\\hat{θ}=\\underset{θ}{argmax}\\ L(θ) = \\underset{θ}{argmax}\\  {\\textstyle \\sum_{i=1}^{|X|}}\\ logP(y_{i}|X_{i};θ)\n",
    "$\n",
    "\n",
    "<font color=blue>generative classifier与discriminate classifier的简单比较:</font>\n",
    "generative models的求解方式很绕，为了求P(y|X)要先求P(X|y),而disciminative model则直接求P(X|y)。但Distriminative model不能使用简单的count-based decomposition。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdae62d",
   "metadata": {},
   "source": [
    "#### 1.3.1 BOW(Bag-of-words)判别模型\n",
    "**二分类问题**：先计算score，再用logistic function转化为probability\n",
    "1. 模型训练阶段：\\\n",
    "用MLE给每个word $x_i$找到$θ_{y|x_i}=P(y|x_i)$ \\\n",
    "这里minL(θ)用Gradient Descent\n",
    "2. 模型推理阶段： \\\n",
    "先计算score：$score_{y|X}=θ_y+{\\textstyle \\prod_{i=1}^{|X|}}θ_{y|x_i} \n",
    "$ \\\n",
    "再转化为probability：$P(y|X;θ)=sigmoid(score_{y|X})=\\frac{1}{1+e^{-score_{y|X}}}$\n",
    "\n",
    "**多分类问题**：先计算score，再用softmax function转化为probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46973a87",
   "metadata": {},
   "source": [
    "## 2. 模型评估\n",
    "### 2.1 extrinsic and intrinsic evaluation\n",
    "1. 有两种评估类别： \\\n",
    "(1) extrinsic evaluation: 针对具体任务跑结果，做对比。就具体任务而言，这种方式的结果更准确，但是耗时，成本高。 \\\n",
    "(2) intrinsic evaluation: 思路是，在训练集上训练，然后在测试集上评估模型质量。因此可以独立于任务来对模型做评估，但是在具体任务上并不是100%准确。\n",
    "2. 语言模型的intrinsic evaluation: **<font color=orange>probability and perplexity</font>**  \\\n",
    "(1) 如何合理的划分训练集和测试集：\\\n",
    "· 为了防止多次使用test set，通常将数据集分成trainning set, dev set和test set。实践中，通常将数据按照8:1:1来划分。 \\\n",
    "· 理论上：test set的大小至少要有足够的statistical power to measure a statistically significant difference between two models。 \\\n",
    "(2) **评价标准：** \\\n",
    "· **思路：**<font color=red>就两种语言模型的比较而言，在测试集上给出的probability更高的那个模型更好。</font>因为，给出的probability越高，意味着更好的预测了test set的出现是合理的。 \\\n",
    "· **metric**：<font color=blue>实践中通常不会直接用probability，而是用perplexity。后者对文本长度做了调整，而且有更好的可解释性。</font> \\\n",
    "(3) <font color=red>模型具有可比性的前提：训练时使用相同的vocabulary，测试时使用相同的test set。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494e4bc",
   "metadata": {},
   "source": [
    "### 2.2 metric of intrinsic evaluation: perplexity(PPL)\n",
    "#### 2.2.1 perplexity的定义\n",
    "1. 定义：X为test set word sequence，N=|X|，有：\n",
    "$$\\begin{eqnarray*}\n",
    "PPL(X) & = & P(x_1, x_2, ..., x_n)^{-1/N}  ...①\\\\\n",
    " & = & [ {\\textstyle \\prod_{i=1}^{N}}\\ P(x_i|x_1:x_{i-1}) ]^{-1/N} ...②\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "2. 与probability的关系： \\\n",
    "$$\\begin{align}\n",
    "          \\underset{θ}{argmin}\\ PPL(X;θ) & = \\underset{θ}{argmax}\\  P(X;θ)^{1/N} \\\\\n",
    "           & = \\underset{θ}{argmax}\\ \\frac{1}{N}  \\ {\\textstyle \\sum_{i=1}^{N}}\\ logP(y_{i}|X_{i};θ)\n",
    "          \\end{align}$$ \\\n",
    "<font color=orange>**· 在模型训练时，MLE的优化目标是找到: $\\hat{θ}=argmaxP(X;θ)$。此时，$P(X;θ)$的值是多少不重要。** </font>\\\n",
    "<font color=green>**· 而在模型评估时，则是将找到的以$\\hat{θ}$为参数的模型用到test set上，看$P(X;θ)$的大小，用它来评价模型。** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7f8cb",
   "metadata": {},
   "source": [
    "#### 2.2.2 perplexity的数值含义\n",
    "1. 在N-gram model中，perplexity在数值上是$P(x_i|x_{i-n+1}:x_{i-1})$的几何平均: $$PPL(X)=[{\\textstyle \\prod_{i=1}^{N}} P(x_i|x_{i-n+1}:x_{i-1})]^{-1/N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c7602",
   "metadata": {},
   "source": [
    "2. <font color=blue>**perplexity也是weighted average branching factor of a language.**</font> 其中，<font color=blue>**branching factor of a language**</font>的定义是：the number of possible next words that can follow any word.即next word的取值可能数。 \\\n",
    "(1) 数学表示：M为branch factor的数量，Pm表示该branch发生的概率，则：\n",
    "$$PPL(X)=P(x_1,x_2,...,x_N)^{-1/N}=[ {\\textstyle \\prod_{m=1}^{M}}\\ P_m^{M*Pm} ]^{-1/M}$$ \\\n",
    "其中，$P_m表示P(x_i=B_m)$，$B_m$表示branch m。\\\n",
    "(2) 证明：简化只证明unigram的情况：\\\n",
    "$$\\begin{align}\n",
    "PPL(X) & =P(x_1,x_2,...,x_N)^{-1/N}  \\\\\n",
    "& = [{\\textstyle \\prod_{i=1}^{N}} P(x_i)]^{-1/N} \\\\\n",
    "& = [{\\textstyle \\prod_{m=1}^{M}}\\ P(x_i=B_m)^{ \\# {1 \\{ x_i=B_m \\}}}]^{-1/N}\\\\\n",
    "& = [{\\textstyle \\prod_{m=1}^{M}}\\ P(x_i=B_m)^\\frac{{ \\# {1 \\{ x_i=B_m \\}}}}{N} ]^{-1} \\\\\n",
    "& = [ {\\textstyle \\prod_{m=1}^{M}}\\ P_m^{Pm} ]^{-1} \\\\\n",
    "& = [ {\\textstyle \\prod_{m=1}^{M}}\\ P_m^{M*Pm} ]^{-1/M}\n",
    "\\end{align}$$\n",
    "(3) <font color=red>**取值大小的含义：其值越小就意味着选择越少，确定性就越高。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd46d4",
   "metadata": {},
   "source": [
    "<font color=lightblue>**eg: 用unigram做数字识别任务:**</font> \\\n",
    "<font color=lightblue>(1) 如果trainning set中每个digit出现的概率相同，即估计值有：$P(x_i=k)=0.1$,其中(k=0, 1, 2, ...9, i=1, 2, ..., N)。则其perplexity正好是其取值可能数10。</font>证明如下：\n",
    "$$\\begin{align}\n",
    "PPL(X) & =P(x_1,x_2,...,x_N)^{-1/N}  \\\\\n",
    "& =[{\\textstyle \\prod_{i=1}^{N}} P(x_i)]^{-1/N} \\\\\n",
    "& = 10\n",
    "\\end{align}$$\n",
    "<font color=lightblue>(2) 如果trainning set分布是$P(x_i=0)=0.91$,$ P(x_i=1)=P(x_i=2)=...=P(x_i=9)=0.01$，test set中$N=100$。则此时可以预计perplexity比上面等概率的时候要低。因为可以确定，大多数时候出现的都是0，确定性高.</font> \\\n",
    "· 可以用两种方式计算perplexity： \\\n",
    "i. 直接按照定义： \\\n",
    "$PPL(X)=P(x_1,x_2,...,x_N)^{-1/N}=(0.91^{91}*0.01^{9})^{-1/100}$ \\\n",
    "ii. 用weighted average branching factor： \\\n",
    "$PPL(X)=[ {\\textstyle \\prod_{m=1}^{M}}\\ P_m^{M*Pm} ]^{-1/M}=(0.91^{10*1*0.91}*0.01^{10*9*0.01})^{-1/10}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d7f4f",
   "metadata": {},
   "source": [
    "#### 2.2.3 perplexity与熵的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe5603",
   "metadata": {},
   "source": [
    "### 2.3 其它指标\n",
    "1. Accuracy：$acc(y,\\hat y)=\\frac{1}{|y|}\\sum_{i=1}^{|y|} Count(y_i=\\hat{y}_i)$ \\\n",
    "<font color=red>问题：如果有出现概率非常小的类型，比如P(y=0)=1%，那么分类器只要全判y=1就有acc=99% </font>\n",
    "2. precision（<font color=orange>**识别出来的目标的正确率**</font>）：估$\\hat{y}=1$的样本中，确实是y=1的比例 \\\n",
    "$prec(y, \\hat{y}) = \\frac{c(y=1, \\hat{y}=1)}{c(\\hat{y}=1)} $\n",
    "3. recall（<font color=orange>**有多少比例的目标被识别**</font>）：所有y=1的样本中，被估为$\\hat{y}=1$的比例\\\n",
    "$rec(y, \\hat{y}) = \\frac{c(y=1, \\hat{y}=1)}{c(y=1)} $\n",
    "4. F-measure(F1 score)：$\\frac{1*prec*rec}{prec+rec}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f51c0",
   "metadata": {},
   "source": [
    "### 2.4 统计检验(significant testing)\n",
    "#### 2.4.1 基本概念\n",
    "1. p-value:\n",
    "2. confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665f4cc",
   "metadata": {},
   "source": [
    "#### 2.4.2 unpaired and paired test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20694b5",
   "metadata": {},
   "source": [
    "#### 2.4.3 bootstrap test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
