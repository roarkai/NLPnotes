{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2d5716",
   "metadata": {},
   "source": [
    "# LM evaluation: Perplexity\n",
    "- ref: chapter3 of SLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46973a87",
   "metadata": {},
   "source": [
    "## I. 评估语言模型的两类方法\n",
    "1. 有两种评估类别：\n",
    "   1. extrinsic evaluation: 针对具体任务跑结果，做对比。就具体任务而言，这种方式的结果更准确，但是耗时，成本高。\n",
    "   2. intrinsic evaluation: 思路是，在训练集上训练，然后在测试集上评估模型质量。因此可以独立于任务来对模型做评估，但是在具体任务上并不是100%准确。\n",
    "\n",
    "2. 语言模型的intrinsic evaluation: **<font color=orange>probability and perplexity</font>**\n",
    "   1. 如何合理的划分训练集和测试集：\n",
    "      - 为了防止多次使用test set，通常将数据集分成trainning set, dev set和test set。实践中，通常将数据按照8:1:1来划分。\n",
    "      - 理论上：test set的大小至少要有足够的statistical power to measure a statistically significant difference between two models。\n",
    "   2. **评价标准：**\n",
    "      - **思路：**<font color=red>就两种语言模型的比较而言，在测试集上给出的probability更高的那个模型更好。</font>因为，给出的probability越高，意味着更好的预测了test set的出现是合理的。\n",
    "      - **metric**：<font color=blue>实践中通常不会直接用probability，而是用perplexity。后者对文本长度做了调整，而且有更好的可解释性。</font>\n",
    "   3. <font color=red>模型具有可比性的前提：训练时使用相同的vocabulary，测试时使用相同的test set。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494e4bc",
   "metadata": {},
   "source": [
    "## II. Intrinsic evaluation metric: perplexity(PPL)\n",
    "### II.1 定义\n",
    "1. 定义：X为test set word sequence，N=|X|，有：\n",
    "$$\\begin{eqnarray*}\n",
    "PPL(X) & = & P(x_1, x_2, ..., x_n)^{-1/N}  ...①\\\\\n",
    " & = & [ {\\textstyle \\prod_{i=1}^{N}}\\ P(x_i|x_1:x_{i-1}) ]^{-1/N} ...②\n",
    "\\end{eqnarray*}$$\n",
    "   - <font color=red>注：实际上Perplexity是用熵定义的，然后根据熵的表达式可以推出上述表达式。后面会介绍真实定义，这里为方便理解，先将Perplexity的数值与语言中的实践含义联系起来。</font>\n",
    "2. 与probability的关系：\n",
    "$$\\begin{align}\\underset{θ}{argmin}\\ PPL(X;θ) & = \\underset{θ}{argmax}\\  P(X;θ)^{1/N} \\\\\n",
    "           & = \\underset{θ}{argmax}\\ \\frac{1}{N}  \\ {\\textstyle \\sum_{i=1}^{N}}\\ logP(y_{i}|X_{i};θ)\\end{align}$$ \n",
    "   - <font color=orange>**在模型训练时，MLE的优化目标是找到: $\\hat{θ}=argmaxP(X;θ)$。此时，$P(X;θ)$的值是多少不重要。** </font>\n",
    "   - <font color=green>**而在模型评估时，则是将找到的以$\\hat{θ}$为参数的模型用到test set上，看$P(X;θ)$的大小，用它来评价模型。** </font>\n",
    "\n",
    "3. perplexity的数值含义\n",
    "   - 在N-gram model中，perplexity在数值上是$P(x_i|x_{i-n+1}:x_{i-1})$的几何平均: $$PPL(X)=[{\\textstyle \\prod_{i=1}^{N}} P(x_i|x_{i-n+1}:x_{i-1})]^{-1/N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34e866b",
   "metadata": {},
   "source": [
    "## II.2 Language Entropy and Perplexity\n",
    "### II.2.1 从Language Entropy到Perplexity\n",
    "#### II.2.1.1 sequence的per word entropy\n",
    "1. 符号\n",
    "   - n个word构成的sequence记为$X=x_1,x_2,...,x_n$\n",
    "   - sequence可以取language L中的任意长为N的sequence，记为$X\\in L$\n",
    "2. 定义：在给定language中，长为n的word sequence的熵为：\n",
    "$$\\begin{align}\n",
    "H(x_1,x_2,...,x_n) & = -\\sum_{X\\in L}P(X)logP(X)\\\\\n",
    "& = -\\sum_{X\\in L}P(x_1,x_2,...,x_n)logP(x_1,x_2,...,x_n)\n",
    "\\end{align}$$\n",
    "3. **sequence entropy**: average entropy per word of a sequence\n",
    "   - 为了抵消sequence长度对熵的影响，定义<font color=blue>**Entropy rate，其含义是average entropy per word**</font>:\n",
    "$$\\begin{align}\n",
    "H(X)& =\\frac{1}{n} H(x_1,x_2,...,x_n)\\\\\n",
    "& = -\\frac{1}{n}\\sum_{X\\in L}P(x_1,x_2,...,x_n)logP(x_1,x_2,...,x_n)\\end{align}$$\n",
    "   - <font color=red>**注意符号：当X表示整个sequence的时候，H(X)是average entropy per word**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec9ebc",
   "metadata": {},
   "source": [
    "4. **language entropy**: average entropy per word of a language\n",
    "   - 如果将语言看做可以用来生成infinite sequence的随机过程L。那么可以将语言的Entropy rate表示为：\n",
    "$$\\begin{align}\n",
    "H(L) & = -\\lim_{n \\to \\infty }\\frac{1}{n}H(x_1, ..., x_n) \\\\\n",
    "& = -\\lim_{n \\to \\infty }\\frac{1}{n}\\sum_{X\\in L}P(x_1,...,x_n)logP(x_1,...,x_n)\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f581cc0",
   "metadata": {},
   "source": [
    "#### II.2.1.2 sequence的熵的性质：Shannon-McMillan-Breiman theorem\n",
    "1. Shannon-McMillan-Breiman theorem：如果language（对应的随机过程）是stationary且ergodic的，则当sequence长度$N\\rightarrow \\infty$时：$$\\begin{align}\n",
    "H(L) = \\lim_{n \\to \\infty }H(X) & = -\\lim_{n \\to \\infty }\\frac{1}{n}\\sum_{X\\in L}P(x_1,...,x_n)logP(x_1,...,x_n)\\\\\n",
    "& = -\\lim_{n \\to \\infty }\\frac{1}{n}logP(x_1,...,x_n)\n",
    "\\end{align}$$\n",
    "2. 对该定理的理解：\n",
    "   - <font color=norange>**ergodic**</font>指：在一个随机过程中，单一样本函数(sample function)包括了整个随机过程的统计特征。它保证了如果有一个long sufficient sequence，那么该sequence的均值会converge to the expected value over all possible sequences.\n",
    "     $$\\lim_{n\\rightarrow \\infty}E(x_i|context)=E(x|context),x_i\\in sample sequence$$\n",
    "   - <font color=norange>**stationary**</font>指：由L生成的word sequence的概率分布不随时间变化。假设Language对应的生成模型是$P(x_i|context_i)$，stationary意味着每个word都是由相同的分布来生成的。\n",
    "     $$P(x_i|context)=P(x_j|context)$$\n",
    "   - <font color=blue>**intuition**</font>：如果language符合这两个性质，那么一个足够长的long sequence中会包含非常多其他短的sequence，并且这些短的sequence会按照他们在语言中的概率分布重复出现在这个long sequence中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9ca6c",
   "metadata": {},
   "source": [
    "3. <font color=green>该定理的用途：将语言的熵转化为长度无限的单个sequence的统计特征。此时可以用样本sequence的统计特征来近似估计语言的熵。</font>\n",
    "   - 如果language对应的随机过程是ergodic的，那么sufficient long sequence能够包含随机过程的统计特征，此时：\n",
    "$$\\begin{align}\n",
    "& -\\lim_{n \\to \\infty }\\frac{1}{n}logP(x_1,...,x_n)  =-\\frac{1}{n}logP(x_1,...,x_n) \\\\\n",
    "& \\therefore H(L) = -\\frac{1}{n}logP(x_1,...,x_n)\n",
    "\\end{align}$$\n",
    "4. 注：<font color=red>Shannon-McMillan-Breiman theorem不同于Monte Carol sampling</font>\n",
    "   - 如果用MC sampling，则表达式中的期望值应该用样本均值表示如下：$$\\begin{align}\n",
    " H(X) & = -\\frac{1}{N}\\sum_{X\\in L} P(x_1, ..., x_N)logP(x_1, ..., x_N)\\\\\n",
    "& = -\\frac{1}{N}\\left[\\frac{1}{M}\\sum_{k=1}^{M}logP(x_{k1}, ..., x_{kN})\\right]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80d613",
   "metadata": {},
   "source": [
    "#### II.2.1.3 sufficiently long sequence的熵的性质\n",
    "- 如果一个language对应的随机过程是ergodic且stationary的，那么用该语言随机生成一个sequence X的per word entropy可以用该语言所生成的一个sufficiently long sequence 样本{x_1, x_2, ..., x_N}的自信息来估计。其值等于随机过程对应的概率分布函数的熵。\n",
    "$$\\begin{align}\n",
    "H(X)&=-\\frac{1}{N}logP(x_1, x_2, ..., x_N)\\\\&=E[-logP(x|context)]\\end{align}$$\n",
    "  - <font color=red>注：这里$X\\ne {x_1, x_2, ..., x_N}$。左边H(X)是随机变量X的分布P(X)对应的熵，X是随机变量而不是具体的某一个sequence，且其长度不一定是N。右边则是某个特定的长为N（sufficiently long）的样本序列(event)的自信息。</font>\n",
    "  - <font color=green>[证明见附录1，该结论证明是rk参考claude完成]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f550f27",
   "metadata": {},
   "source": [
    "#### II.2.1.4 Perplexity的正式定义\n",
    "$$\\begin{align}\n",
    "Perplexity(X)& =2^{ H(X)}\\\\\n",
    " & = 2^{-\\frac{1}{N}logP(x_{1}, x_2, ..., x_N)}\\\\\n",
    "& = P(x_1, x_2, ..., x_N)^{-\\frac{1}{N}}\\\\\n",
    "& = \\left( \\prod_{i=1}^N P(x_i|x_1, ..., x_{i-1})\\right )^{-\\frac{1}{N}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504af660",
   "metadata": {},
   "source": [
    "### II.2.2 熵、Perplexity在语言模型中的实践含义\n",
    "#### II.2.2.1 H(x)是随机变量x的平均最优码长\n",
    "1. 熵是随机变量自信息的期望值，是用bit为单位计量的随机变量所携带的自信息的平均大小。\n",
    "   $$H(x)=-\\sum_{x\\in \\mathcal{X}}P(x)log_2P(x)$$\n",
    "2. **事件x**的自信息是事件x的最优编码长度。如果对随机变量取值空间中的每个event都用最优编码，那么该随机变量(用bit为单位计量)的平均码长等于该随机变量的熵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ee78a",
   "metadata": {},
   "source": [
    "#### II.2.2.2 effective branching factor = $2^{H(x)}$\n",
    "1. **随机变量的effective number of equally value**定义\n",
    "   - 离散随机变量x的事件空间为$\\mathcal{X}$，事件空间中event数$|\\mathcal{X}|=k$.（简化假设$k=2^d$，d是整数，因此没有round up to integer的问题）。当x是等概率分布时，熵最大，值为$H(x)=-log\\frac{1}{k}=logk=d$。\n",
    "   - 可见，H(x)=d对应着：有$2^d$种取值的等概率分布。将$2^d$称为<font color=norange>分布的**effective number of equally probable choices**</font>，也称为<font color=norange>分布的**effective braching factor**</font>\n",
    "2. 理解：\n",
    "   - 当分布的熵为d时，其信息量等价于一个有$2^d$种等概率取值的分布所含信息量。\n",
    "3. 与Perplexity的关系：Perplexity=effective number of equally probable choices\n",
    "   - 因为$Perplexity(x)=2^{H(x)}$，因此当x是一个离散随机变量时，它的Perplexity的值就是其分布的effective number of equally probable choices。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e3713",
   "metadata": {},
   "source": [
    "4. 在语言环境中的含义\n",
    "   - **单个word**：在语言中，$P(x|context)$的熵为$H(x|context)$，对应的$Perplexity(x|context)=2^H(x|context)$，其含义为基于context来生成next word时，有$2^{H(x)}$个等概率的next word。<font color=deeppink>**$2^{H(x)}$ is the effective number of choices that the model faces when predicting the next word.**</font>\n",
    "   - **word sequence**：长为N的word sequence X，其分布P(X)的熵为H(X)。\n",
    "     - 分析：$$\\begin{align}\n",
    "Perplexity(X) & = 2^{H(X)} \\\\\n",
    "& = 2^{\\frac{1}{N}H(x_1, x_2, ..., x_N)} \\\\\n",
    "& = 2^{\\frac{1}{N}[H(x_1)+H(x_2|x_1)+ ...+H(x_N|x_1,...,x_{N-1})]} \\\\\n",
    "& = \\left (2^{[H(x_1)+H(x_2|x_1)+ ...+H(x_N|x_1,...,x_{N-1})]}\\right)^{\\frac{1}{N}} \\\\\n",
    "& = \\left (2^{H(x_1)}*2^{H(x_2|x_1)}*...*2^{H(x_N|x_1,...,x_{N-1})}\\right)^{\\frac{1}{N}} \\\\\n",
    "& = \\left [ ppl(x_1)* ppl(x_2)* ... * ppl(x_N)\\right ]^{\\frac{1}{N}} \\\\\n",
    "\\end{align}$$\n",
    "       - 可见，<font color=blue>Perplexity of a sequence等于单个word的Perplexity的几何平均。</font>\n",
    "       - 从Perplexity的实践含义来看，单个word的Perplexity表示the effective number of choices of one word，他们的几何平均就是**the average of the effective number of choices of the words in the sequence**.也称为<font color=norange>**average effective braching factor of the words in the sequence**</font>.\n",
    "       - <font color=green>**Perplexxity of word sequence**: the effective number of choices a model faces when predicting the next word, averaged over all positions in a sequence.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d9ddd",
   "metadata": {},
   "source": [
    "- 引例1：<font color=orange>假设有一种由A/B/C/D四个值构建的语言。在该语言生成的序列中，$P(x_i)$独立同分布，且$P(x_i=A)=P(x_i=B)=P(x_i=C)=P(x_i=D)=\\frac{1}{4}$。那么该语言的branching factor就是4。</font>注：4只是branching factor，不是average weighted branching factor。\n",
    "- 引例1续：<font color=orange>此时该语言的PPL=4。</font>\n",
    "  - 证明：\n",
    "      $$\\begin{align}\n",
    "H(x_i) & = -4*(\\frac{1}{4}*log\\frac{1}{4})=2 \\\\\n",
    "Perplexity(X) & = \\left [ ppl(x_1)* ppl(x_2)* ... * ppl(x_N)\\right ]^{\\frac{1}{N}} \\\\\n",
    "& = \\left(4^N \\right)^{\\frac{1}{N}} \\\\\n",
    "& = 4\n",
    "\\end{align}$$\n",
    "- 引例2：<font color=orange>如果trainning set和test set中每个digit出现的概率改为$P(x_i=A)=\\frac{1}{2},P(x_i=B)=\\frac{1}{4},P(x_i=C)=P(x_i=D)=\\frac{1}{8}$。则此时可以预计perplexity比上面等概率的时候要低。因为大多数时候出现的都是A，确定性高.</font>\n",
    "  - 分析PPL：\n",
    "      $$\\begin{align}\n",
    "H(x_i) & = -\\left(\\frac{1}{2}*log\\frac{1}{2}+ \\frac{1}{4}*log\\frac{1}{4}+ \\frac{1}{8}*log\\frac{1}{8} + \\frac{1}{8}*log\\frac{1}{8}\\right )=\\frac{7}{4}  \\\\\n",
    "Perplexity(X) & = \\left [ ppl(x_1)* ppl(x_2)* ... * ppl(x_N)\\right ]^{\\frac{1}{N}} \\\\\n",
    "& = \\left(2^{\\frac{7}{4}N} \\right)^{\\frac{1}{N}} \\\\\n",
    "& = 2^{\\frac{7}{4}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806e2d6",
   "metadata": {},
   "source": [
    "### 附录\n",
    "#### 1. 证明sequence entropy的性质\n",
    "- <font color=brown>ref: claude</font>\n",
    "\n",
    "- 如果一个language对应的随机过程是ergodic且stationary的，那么用该语言随机生成一个sequence X的per word entropy可以用该语言所生成的一个sufficiently long sequence 样本{x_1, x_2, ..., x_N}来估计。\n",
    "$$H(X)=-\\frac{1}{N}logP(x_1, x_2, ..., x_N)$$\n",
    "  - <font color=red>注：这里$X\\ne {x_1, x_2, ..., x_N}$。左边H(X)是随机变量X的分布P(X)对应的熵，X是随机变量而不是具体的某一个sequence，且其长度不一定是N。右边则是某个特定的长为N的样本序列(event)的自信息。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53308f",
   "metadata": {},
   "source": [
    "- 推导：\n",
    "  1. 根据sequence entropy的定义：$$\\begin{align}H(X) & = \\frac{1}{n}H(x_1,x_2,...,x_n)\\\\\n",
    "& = -\\frac{1}{n}\\sum_{X\\in L}P(x_1,x_2,...,x_n)logP(x_1,x_2,...,x_n)\\\\\n",
    "& = -\\frac{1}{n}\\sum_{X\\in L}P(x_1,x_2,...,x_n)\\sum_{i=1}^NlogP(x_i|x_1, ..., x_{i-1})\\\\\n",
    "& = -\\frac{1}{n}\\sum_{i=1}^N\\left[\\sum_{X\\in L}P(x_1,x_2,...,x_{\\color{red} n} )logP(x_i|x_1, ..., x_{i-1})\\right]\\\\\n",
    "& = -\\frac{1}{n}\\sum_{i=1}^N\\left[\\sum_{X\\in L}P(x_1,x_2,...,x_{\\color{red} i} )logP(x_i|x_1, ..., x_{i-1})\\right]\\\\\n",
    " \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16cb6e9",
   "metadata": {},
   "source": [
    "2. 如果<font color=red>language对应的随机过程</font>是stationary且ergodic的，那么：$$\\begin{align}\n",
    "P(x_i|x_1,...,x_{i-1}) & =P(x_j|x_1,...,x_{j-1}),\\forall j\\ne i\n",
    "\\end{align}$$\n",
    "   - 此时有：$$H(X) = E[-logP(x|context)]...[1]$$\n",
    "   - 证明：$$\\begin{align}\n",
    "H(X) & = -\\frac{1}{n}\\sum_{i=1}^n\\left[\\sum_{X\\in L}P(x_1,x_2,...,x_i )logP(x_i|x_1, ..., x_{i-1})\\right]\\\\\n",
    "& = -\\frac{1}{n}*n\\left[\\sum_{X\\in L}P(x_1,x_2,...,x_i )logP(x_i|x_1, ..., x_{i-1})\\right]\\color{Green} {\\because stationary }\\\\\n",
    "& = -\\sum_{X\\in L}P(x_1,x_2,...,x_i )logP(x_i|x_1, ..., x_{i-1})\\\\\n",
    "& = E[-logP(x_i|x_1, ..., x_{i-1})]\\\\\n",
    "& = E[-logP(x_i|context)]\\\\\n",
    "& = E[-logP(x|context)] \\color{Green} {\\because ergodic}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8ff44",
   "metadata": {},
   "source": [
    "3. 如果生成<font color=red>特定的长为N(sufficiently long)的sequence ${x_1, ..., x_N}$</font>的随机过程是stationary且ergodic的，可以证明：$$-\\lim_{N \\to \\infty}\\frac{1}{N}logP(x_1, ..., x_N) = E[-logP(x|context)]...[2]$$\n",
    "   - 证明：$$\\begin{align}\n",
    "-\\lim_{N \\to \\infty}\\frac{1}{N}logP(x_1, ..., x_N) & = -\\lim_{N \\to \\infty}\\frac{1}{N}\\sum_{i=1}^NlogP(x_i|x_1, ..., x_{i-1}) \\\\\n",
    "& = -\\lim_{N \\to \\infty}\\frac{1}{N}\\sum_{k}m_klogP(x_k|context_k)\\\\\n",
    "& = -\\lim_{N \\to \\infty}\\sum_{k}\\frac{m_k}{N}logP(x_k|context_k)   \\end{align}$$k是sequence中不同$word|context$组合的index,有的$word|context$会重复出现，因此用$m_k$记录$x_k|context_k$出现的次数。\n",
    "   - 根据大数定律，有$N\\rightarrow \\infty$时，$\\frac{m_k}{N}=P(x_k|context_k)$\n",
    "   - 因为sequence X是sufficiently long的，且生成它的stochastic process是ergodic的，因此N中不同的$x_k|context_k$包含了所有language中可能的$word|context$组合。\n",
    "\n",
    "   - 代入上式：$$\\begin{align}\n",
    "& -\\lim_{N \\to \\infty}\\frac{1}{N}logP(x_1, ..., x_N) \\\\\n",
    "& = -\\lim_{N \\to \\infty}\\sum_{k}\\frac{m_k}{N}logP(x_k|context_k)   \\\\\n",
    "& = -\\lim_{N \\to \\infty}\\sum_{k}P(x_k|context_k)logP(x_k|context_k) \\color{Green} {\\because 大数定律}   \\\\\n",
    "& = E[-logP(x_i|context)] \\color{Green} {\\because ergodic} \\\\\n",
    "& = E[-logP(x|context)]  \\color{Green} {\\because stationary} \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419473f",
   "metadata": {},
   "source": [
    "4. 根据[1]和[2]有：\n",
    "$$\\begin{align}\n",
    "H(X) & =-\\lim_{N \\to \\infty}\\frac{1}{N}logP(x_1, ..., x_N) \\\\\n",
    "& \\approx -\\frac{1}{N}logP(x_1, ..., x_N) \n",
    "\\end{align}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
