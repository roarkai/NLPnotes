{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab85e89",
   "metadata": {},
   "source": [
    "# word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a6568",
   "metadata": {},
   "source": [
    "## I. representation learning\n",
    "1. representation learning \\\n",
    "word vector是representation learning的一种。通过self-supervised方法来学习inputs的representation，而不是手动通过feature engineering来创建representation of inputs。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3ccf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T01:53:33.807363Z",
     "start_time": "2023-12-15T01:53:33.795578Z"
    }
   },
   "source": [
    "2. model of word meaning: 一个好的model of word meaning应该可以较好的表达词汇的含义在语义空间的分布。比如能够表达：\\\n",
    "① 近义词、反义词。 \\\n",
    "② 词语的感情色彩：褒义和贬义 \\\n",
    "③ 同属于一簇的词汇（如：buy，sell，pay都是购物场景的词汇；cat，dog都是动物）\\\n",
    "更一般地说，一个好的model of word meaning应该有助于解决meaning-related task。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba7119",
   "metadata": {},
   "source": [
    "3. lexical semantics（词汇语义） \\\n",
    "⑴ lemma和wordform：lemma是词元、词根，wordform是用词根生成的具体的词形。每个lemma有多种含义（word sense）\\\n",
    "⑵ synonyms同义词: \\\n",
    "① synonym between word senses：指1个word的1种word sense与另一个word的1种word sense相同。这是可能发生的。\\\n",
    "② synonym between words：指两个word含义相同可以互相替代。<font color=red>但实际上没有真正的synonym between words，water和$H_2O$指同样的东西，但使用场景不同。</font> \\\n",
    "<font color=green>**在representation learning中通常关注的是word similarity**：在word sense的层面建模的话，同一个word有不同的representation，实际任务中很难处理。如果改在word层面建模，因为没有真正的word synonym，关注synonym没有意义，所以关注word similarity是更符合word真实情况的方式。</font>\\\n",
    "⑶ word relatedness: 词与词之间的关系不止similarity。还有很多其他关系，如：\\\n",
    "① sematic field：一种典型的词之间的relatedness是<font color=blue>**词语有相同的sematic field。**</font>比如：waiter,menu,plate,food, chef都属于sematic field of restaurant，而且这些词本身的词义没有similarity。\\\n",
    "② sematic frame：一组含有特定事件（event）信息的词，不同的词在事件中的角色不同。比如，以transaction这个event为例，会有seller, buyer, pay, money, goods等角色，而不同的words会承担这些角色。如果representation可以表达出这部分内容的话，就能识别出Bob bought a desk from Jim. Jim sold his desk to Bob.这两句话含义相同。\\\n",
    "③ connotation: 语言中的情绪，包括：emotion, sentiment, evaluation, or opinion. \\\n",
    "④ 其他，反义词、上义词、下义词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9d1ae",
   "metadata": {},
   "source": [
    "4. vector sematics \\\n",
    "⑴ 含义：指的是用vector表示word，每个word是多维语义空间中的一个点。这个语义空间是利用语料中的word disrtibution信息得到的。 \\\n",
    "⑵ 优点：可以用无监督学习自动从语料中获得vector sematic \\\n",
    "⑶ 方法：两种最基础的方法是tf-idf和ppmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220abefe",
   "metadata": {},
   "source": [
    "## II. co-occurrence matrix\n",
    "### II.1 term-document matrix\n",
    "1. 结构: 每行表示词汇表中的1个word，每列对应corpus中的1个document，table中第i行j列的value是：$$v_{i,j}=\\# word_i\\ in\\ doc_j $$\n",
    "表示word i在document j中出现的次数。word对应的row value作为该word的vector。\n",
    "2. intuition： \\\n",
    "① 用document中包含的word信息来表达document。 \\\n",
    "② 如果两个doc中出现的word相似，那么这两个doc的内容会更相似。可以用来做信息检索。\\\n",
    "③ 用word出现的document信息来表达word meaning。 \\\n",
    "④ <font color=red>如果两个word出现在类似的doc中，说明这两个word相似度高。</font>\n",
    "3. vector的数值特点：sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b7f129",
   "metadata": {},
   "source": [
    "### II.2 term-term matrix (或word-word matrix)\n",
    "1. 结构: 每行每列都对应词汇表中的1个word，table中第i行j列的value是：$$v_{i,j}=\\# word_j\\ in\\ window\\ of\\ word_i $$\n",
    "行中的word是target word，列中的word是context word。table中第i行j列的值是整个语料中，context word出现在target word的context window中的次数。\n",
    "2. intuition： \\\n",
    "① 用word上下文窗口中的其他word信息，也即co-occurrence count来表达word meaning。 \\\n",
    "② <font color=red>如果两个word总是出现在彼此的context window中，对应位置的co-occurrence count value大，说明这两个word关系紧密。</font>\n",
    "3. vector的数值特点：sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4348e274",
   "metadata": {},
   "source": [
    "### II.3 measuring similarity and relatedness\n",
    "#### 1. inner product\n",
    "⑴ 已知documents的数量有N个，每个word用N维向量表示，两个word v和w之间的相似性可以用他们vector的inner product来衡量：\n",
    "$$<v, w>= {\\textstyle \\sum_{i=1}^{N}}v_iw_i$$\n",
    "当vector来自term-document matrix时，N=|D|；来自term-term matrix时，N=|V|。\\\n",
    "⑵ 问题：如果vector中非零元素的个数越多，那么inner product的结果就越大。这意味着：\n",
    "① term-document matrix中，常见高频词汇（the, it等）出现的doc多，非零元素多，因此他与其他word的inner product值偏大。\\\n",
    "② term-term matrix中，高频词汇与更多词汇有co-occurrence，且总的来说与单个context word的co-occurrence次数也多，所以inner product偏大。\\\n",
    "上面两种情况并不表示较高的similarity或者relatedness，这些高频词汇提供的信息本身也有限。\n",
    "#### 2. cosine\n",
    "⑴ 针对高频词汇带来的similarity度量不准确的问题，可以对inner product做normalization：$$\\frac{v·w}{|v||w|}=\\cos \\theta$$\n",
    "⑵ 含义：此时向量之间的夹角表示两个word的relatedness \\\n",
    "⑶ <font color=red>[rk's note]这里修正的方法也许可以有其他方式。比如不用向量单位化$v^{'}=\\frac{v}{|v|}$，而用$v^{'}=\\frac{v}{ {\\textstyle \\sum_{i=1}^{N}} v_i}$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5949a170",
   "metadata": {},
   "source": [
    "## III. 对co-occurrence matrix做权重修正\n",
    "1. **背景**：简单的co-occurrence count无法很好地度量words之间的relatedness，一个很大的问题还是来自常见高频词。\\\n",
    "① 在term-document matrix中，如果word经常出现在某个doc中，但不经常出现在别的doc中，这个word对于标记该doc会很重要；如果word经常出现在所有doc中，那么它对于标记doc就不重要。\\\n",
    "② 在term-term matrix中，如果word经常出现在特定word的context window中，但不是经常出现在大量的words的context window中，那么前一种word所表达的信息就比后一种更多。\n",
    "2. <font color=green>**solution**：给matrix中的value加上weights，让信息量大的words的weights大，信息量小的常见高频词的weights小，用weights修正有信息词和无信息高频词的vector value，实现两种情况的区分。\\\n",
    "tf-idf weights就是处理term-document matrix的weights，ppmi则是处理term-term matrix的weights。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f3589",
   "metadata": {},
   "source": [
    "### III.1 TF-IDF\n",
    "1. 方法 \n",
    "$$\\begin{align} \n",
    "  w_{t,d} & = tf_{t, d}*idf_t \\\\\n",
    "\\\\\n",
    "其中， tf_{t, d} & = log_{10}(count(t,d)+1)\\\\\n",
    "idf_t & = log_{10}(\\frac{N}{df_t})\\\\\n",
    " df_t & =  {\\textstyle \\sum_{i=1}^{N}}\\#1\\{word_t\\ in\\ doc_i\\} \n",
    "\\end{align}$$\n",
    "2. 思路 \\\n",
    "⑴ <font color=blue>$tf_{t,d}$是term frequency of word t in document d.</font>\\\n",
    "① 不用raw frequency，因为出现100次的word对doc的重要性不太可能是出现1次的词的100倍，所以用$log_{10}$来降低数量级。\\\n",
    "② 加1是为了避免log0 \\\n",
    "⑵ <font color=blue>$idf_t$是inverse document frequency。</font> \\\n",
    "① 如果一个词出现在很多doc中，那么它的idf就会更低。\\\n",
    "② 由于N是doc的数量，这个值通常都很大，所以也用log来降低数量级\n",
    "3. 应用场景：信息检索中用的多"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9b6e6",
   "metadata": {},
   "source": [
    "### III.2 PPMI (positive pointwise mutual information)\n",
    "1. 直觉：衡量两个word之间相关性的最好方式是，先计算假如完全是by chance的话，两个word的co-occurrence的概率，然后看他们实际co-occurrence的概率。如果超过by chance的话，那么他们就相关，超过越多，相关性越强。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38eda4",
   "metadata": {},
   "source": [
    "2. pointwise mutual information：$$I(x, y)=log_2\\frac{P(x, y)}{P(x)P(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09ba148",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96bb9e1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41d72b83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87a3d47b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a59960fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "761e1e4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08c22ddf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
