{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c54a2f5",
   "metadata": {},
   "source": [
    "Certainly! Let's prove that for a long enough sequence W = {w₁, w₂, ..., wN}, the entropy H(W) is equal to -1/N * log P(w₁, ..., wN). We'll do this step-by-step.\n",
    "\n",
    "1) First, let's define entropy:\n",
    "\n",
    "   $H(W) = -∑ P(W) log P(W)$\n",
    "\n",
    "   where the sum is over all possible sequences W of length N.\n",
    "\n",
    "2) For a specific sequence W = {w₁, w₂, ..., wN}, we can write:\n",
    "\n",
    "   $P(W) = P(w₁, w₂, ..., wN)$\n",
    "\n",
    "3) Using the chain rule of probability, we can decompose this:\n",
    "\n",
    "   $P(w₁, w₂, ..., wN) = P(w₁) * P(w₂|w₁) * P(w₃|w₁,w₂) * ... * P(wN|w₁,...,wN-1)$\n",
    "\n",
    "4) Taking the log of both sides:\n",
    "\n",
    "   $log P(w₁, w₂, ..., wN) = log P(w₁) + log P(w₂|w₁) + log P(w₃|w₁,w₂) + ... + log P(wN|w₁,...,wN-1)$\n",
    "\n",
    "5) Multiplying both sides by -1/N:\n",
    "\n",
    "   $-1/N * log P(w₁, w₂, ..., wN) = -1/N * [log P(w₁) + log P(w₂|w₁) + log P(w₃|w₁,w₂) + ... + log P(wN|w₁,...,wN-1)]$\n",
    "\n",
    "6) The right-hand side of this equation is the average of the negative log probabilities of each word given its context.\n",
    "\n",
    "7) Now, let's consider what happens as N becomes very large. By the law of large numbers, this average will converge to its expected value:\n",
    "\n",
    "   $lim_{N→∞} -1/N * log P(w₁, w₂, ..., wN) = E[-log P(wi|context)]$\n",
    "\n",
    "   where the expectation is taken over the true distribution of words and contexts in the language.\n",
    "\n",
    "8) This expectation is precisely the definition of entropy for a stationary ergodic process, which is a reasonable assumption for natural language:\n",
    "\n",
    "   $H(W) = E[-log P(wi|context)]$\n",
    "\n",
    "9) Therefore, for a sufficiently long sequence:\n",
    "\n",
    "   $H(W) ≈ -1/N * log P(w₁, w₂, ..., wN)$\n",
    "\n",
    "   And this approximation becomes exact in the limit as N approaches infinity.\n",
    "\n",
    "This proof shows that for a long enough sequence, the entropy H(W) is indeed equal to -1/N * log P(w₁, ..., wN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12104f3",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "&H(W) = -\\sum P(W) \\log P(W) \\\\\n",
    "&P(w_1, w_2, ..., w_N) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot ... \\cdot P(w_N|w_1,...,w_{N-1}) \\\\\n",
    "\\\\\n",
    "&\\log P(w_1, w_2, ..., w_N)= \\log P(w_1) + \\log P(w_2|w_1) + \\log P(w_3|w_1,w_2) + ... \\\\\n",
    "\\\\\n",
    "&-\\frac{1}{N} \\log P(w_1, w_2, ..., w_N) = -\\frac{1}{N} [\\log P(w_1) + \\log P(w_2|w_1) + \\log P(w_3|w_1,w_2) + ...  \\\\\n",
    "\\\\\n",
    "&\\lim_{N \\to \\infty} -\\frac{1}{N} \\log P(w_1, w_2, ..., w_N) = E[-\\log P(w_i|\\text{context})] = H(W)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bcfa7a",
   "metadata": {},
   "source": [
    "Certainly. Let's prove why the limit as N approaches infinity of $-1/N * log P(w₁, w₂, ..., wN) = E[-log P(wi|context)]$. This proof relies on the law of large numbers and the assumption that language can be modeled as a stationary ergodic process.\n",
    "\n",
    "Proof:\n",
    "\n",
    "1) First, let's expand -1/N * log P(w₁, w₂, ..., wN) using the chain rule of probability:\n",
    "\n",
    "   $-1/N * log P(w₁, w₂, ..., wN) = -1/N * [log P(w₁) + log P(w₂|w₁) + log P(w₃|w₁,w₂) + ... + log P(wN|w₁,...,wN-1)]$\n",
    "\n",
    "2) We can rewrite this as an average:\n",
    "\n",
    "   $-1/N * log P(w₁, w₂, ..., wN) = 1/N * ∑(i=1 to N) [-log P(wi|w₁,...,wi-1)]$\n",
    "\n",
    "3) Now, let's consider the random variable Xi = -log P(wi|w₁,...,wi-1). This is the negative log probability of the i-th word given its context.\n",
    "\n",
    "4) Under the assumption of a stationary process, the distribution of Xi is the same for all i, although the actual values may depend on the specific context.\n",
    "\n",
    "5) The ergodic assumption implies that time averages converge to ensemble averages. In other words, the average over a long sequence will converge to the expected value over all possible sequences.\n",
    "\n",
    "6) By the law of large numbers, for a sequence of identically distributed random variables Xi, we have:\n",
    "\n",
    "   $lim_{N→∞} 1/N * ∑(i=1 to N) Xi = E[X]$\n",
    "\n",
    "   where E[X] is the expected value of X.\n",
    "\n",
    "7) In our case, this means:\n",
    "\n",
    "   $lim_{N→∞} 1/N * ∑(i=1 to N) [-log P(wi|w₁,...,wi-1)] = E[-log P(wi|context)]$\n",
    "\n",
    "8) The left side of this equation is exactly our limit from step 2, and the right side is the definition of entropy for a stationary ergodic process.\n",
    "\n",
    "Therefore, we have proven that:\n",
    "\n",
    "$lim_{N→∞} -1/N * log P(w₁, w₂, ..., wN) = E[-log P(wi|context)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed9b21",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "&-\\frac{1}{N} \\log P(w_1, w_2, ..., w_N) \\\\\n",
    "&= -\\frac{1}{N} [\\log P(w_1) + \\log P(w_2|w_1) + \\log P(w_3|w_1,w_2) + ... + \\log P(w_N|w_1,...,w_{N-1})] \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N [-\\log P(w_i|w_1,...,w_{i-1})] \\\\\n",
    "&\\text{Let } X_i = -\\log P(w_i|w_1,...,w_{i-1}) \\\\\n",
    "&\\text{By the Law of Large Numbers:} \\\\\n",
    "&\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{i=1}^N X_i = E[X] \\\\\n",
    "&\\text{Therefore:} \\\\\n",
    "&\\lim_{N \\to \\infty} -\\frac{1}{N} \\log P(w_1, w_2, ..., w_N) = E[-\\log P(w_i|\\text{context})]\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0a7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fdeab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d4eb50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5f855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43addb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee4da1ef",
   "metadata": {},
   "source": [
    "**prove $H(W)=E[-log P(wi|context)]$ for a stationary ergodic process**\n",
    "\n",
    "Proof:\n",
    "\n",
    "1) First, let's recall the definition of entropy for a discrete random variable X:\n",
    "\n",
    "   $H(X) = -∑ P(x) log P(x)$\n",
    "\n",
    "   where the sum is over all possible values x that X can take.\n",
    "\n",
    "2) In the context of a sequence of words W = {w₁, w₂, ..., wN}, we're interested in the entropy rate, which is the per-symbol entropy as the sequence length approaches infinity:\n",
    "\n",
    "   $H(W) = lim(N→∞) 1/N * H(w₁, w₂, ..., wN)$\n",
    "\n",
    "3) Using the chain rule of entropy, we can decompose H(w₁, w₂, ..., wN):\n",
    "\n",
    "   $H(w₁, w₂, ..., wN) = H(w₁) + H(w₂|w₁) + H(w₃|w₁,w₂) + ... + H(wN|w₁,...,wN-1)$\n",
    "\n",
    "4) Substituting this into our entropy rate formula:\n",
    "\n",
    "   $H(W) = lim(N→∞) 1/N * [H(w₁) + H(w₂|w₁) + H(w₃|w₁,w₂) + ... + H(wN|w₁,...,wN-1)]$\n",
    "\n",
    "5) Now, let's consider the definition of conditional entropy:\n",
    "\n",
    "   $H(X|Y) = -∑∑ P(x,y) log P(x|y)$\n",
    "\n",
    "6) Applying this to our word sequence:\n",
    "\n",
    "   $H(wi|w₁,...,wi-1) = -∑ P(wi,w₁,...,wi-1) log P(wi|w₁,...,wi-1)$\n",
    "\n",
    "7) Under the assumption of a stationary process, this conditional entropy is the same for all i (although the actual contexts and words may differ).\n",
    "\n",
    "8) Therefore, as N approaches infinity, our entropy rate becomes:\n",
    "\n",
    "   $H(W) = lim(N→∞) 1/N * N * H(wi|context) = H(wi|context)$\n",
    "\n",
    "9) Now, let's expand H(wi|context):\n",
    "\n",
    "   $H(wi|context) = -∑∑ P(wi,context) log P(wi|context)$\n",
    "\n",
    "10) This can be rewritten as an expectation:\n",
    "\n",
    "    $H(wi|context) = E[-log P(wi|context)]$\n",
    "\n",
    "    where the expectation is taken over all possible words and contexts.\n",
    "\n",
    "Therefore, we have proven that:\n",
    "\n",
    "H(W) = E[-log P(wi|context)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b27e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a843c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf66434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2f3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fdbf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7a5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf826100",
   "metadata": {},
   "source": [
    "1. Stationary Process:A stationary process is one where the statistical properties do not change over time. In the context of our language model:\n",
    "\n",
    "- Role: It ensures that P(wi|context) has the same distribution for all i.\n",
    "- Importance: This allows us to treat -log P(wi|w1,...,wi-1) as identically distributed random variables for different i, which is crucial for applying the law of large numbers.\n",
    "\n",
    "2. Ergodic Process:An ergodic process is one where the time average of a sequence converges to the same value as the ensemble average.\n",
    "\n",
    "- Role: It guarantees that the average over a single long sequence will converge to the expected value over all possible sequences.\n",
    "- Importance: This allows us to equate the limit of the time average (left side of the equation) with the ensemble average (right side of the equation).\n",
    "\n",
    "3. Law of Large Numbers (LLN):The LLN states that the average of a large number of identically distributed random variables converges to their expected value.\n",
    "\n",
    "- Role: It provides the mathematical foundation for the convergence of the sample mean to the expected value.\n",
    "- Importance: It justifies the limit operation, showing that as N approaches infinity, the average converges to the expected value.\n",
    "\n",
    "Now, let's see how these concepts work together in the proof:\n",
    "\n",
    "1. Define Xi = -log P(wi|w1,...,wi-1)\n",
    "\n",
    "2. By stationarity, these Xi are identically distributed for all i.\n",
    "\n",
    "3. The left side of the equation, 1/N * ∑(i=1 to N) Xi, is the sample mean of these Xi.\n",
    "\n",
    "4. By the Law of Large Numbers:\n",
    "   lim(N→∞) 1/N * ∑(i=1 to N) Xi = E[X]\n",
    "\n",
    "5. The ergodic property ensures that this E[X] is the same whether we consider it over time (a single long sequence) or over the ensemble of all possible sequences.\n",
    "\n",
    "6. Therefore, E[X] = E[-log P(wi|context)], where the right side is the ensemble average over all possible words and contexts.\n",
    "\n",
    "In LaTeX, we can express this as:\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\text{Let } X_i = -\\log P(w_i|w_1,...,w_{i-1}) \\\\\n",
    "&\\text{By stationarity, } X_i \\text{ are identically distributed} \\\\\n",
    "&\\text{By the Law of Large Numbers:} \\\\\n",
    "&\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{i=1}^N X_i = E[X] \\\\\n",
    "&\\text{By ergodicity:} \\\\\n",
    "&E[X] = E[-\\log P(w_i|\\text{context})] \\\\\n",
    "&\\text{Therefore:} \\\\\n",
    "&\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{i=1}^N [-\\log P(w_i|w_1,...,w_{i-1})] = E[-\\log P(w_i|\\text{context})]\n",
    "\\end{align*}\n",
    "$$\n",
    "In summary:\n",
    "- Stationarity allows us to treat the terms in the sum as identically distributed.\n",
    "- The Law of Large Numbers provides the convergence of the sample mean to the expected value.\n",
    "- Ergodicity ensures that this convergence is to the ensemble average, allowing us to interpret the result in terms of the overall language statistics rather than just a single sequence.\n",
    "\n",
    "Without any of these three properties, the proof would not hold:\n",
    "- Without stationarity, we couldn't treat the terms as identically distributed.\n",
    "- Without the LLN, we couldn't justify the convergence of the average.\n",
    "- Without ergodicity, the limit might converge to a value specific to a particular sequence rather than the ensemble average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3925819d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ce258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "443ef938",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T01:54:34.965557Z",
     "start_time": "2024-10-02T01:54:34.959814Z"
    }
   },
   "source": [
    "### II.2.3 语言的熵与PPL\n",
    "1. 用Shannon-McMillan-Breiman theorem构建一个简单分布Q来近似语言的分布P\n",
    "   - 假设\n",
    "     - W是给定language中长度为N的word sequence样本。\n",
    "     - 假设一个满足stationary和ergodic特征的随机过程，它对应的概率分布是$Q=P(w_i|w_{i-N+1}:w_{i-1})$。\n",
    "     - 对Q而言，N足够长(sufficiently long)以至于包含了分布Q中的statistical feature。\n",
    "\n",
    "     $$\\begin{align}\n",
    " H(W) & = -\\frac{1}{N}\\sum_{W\\in L} P(w_1, ..., w_N)logP(w_1, ..., w_N)\\\\\n",
    "& = -\\frac{1}{N}logP(w_1, ..., w_N)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e368f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T01:54:44.438354Z",
     "start_time": "2024-10-02T01:54:44.429154Z"
    }
   },
   "source": [
    "\n",
    "   - P与Q的交叉熵为：$$\\begin{align}\n",
    "H(P,Q) & = -\\sum_X P(X)logQ(X) \\\\\n",
    "\\lim_{n \\to \\infty }\\frac{1}{n}H(P,Q)& = -\\lim_{n \\to \\infty } \\frac{1}{n}\\sum_{X\\in L} P(X)logQ(X) \\\\\n",
    "& =-\\lim_{n \\to \\infty } \\frac{1}{n}\\sum_{(x_1,...,x_n)\\in L} P(x_1,...,x_n)logQ(x_1,...,x_n)\\end{align}$$\n",
    "   - 根据Shannon-McMillan-Breiman theorem：$$\\begin{align}\n",
    "\\lim_{n \\to \\infty }\\frac{1}{n}H(P,Q) & = -\\lim_{n \\to \\infty } \\frac{1}{n}\\sum_{(x_1,...,x_n)\\in L} P(x_1,...,x_n)logQ(x_1,...,x_n) \\\\\n",
    "& = -\\lim_{n \\to \\infty } \\frac{1}{n}logQ(x_1,...,x_n) \\\\\n",
    "\\end{align}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
