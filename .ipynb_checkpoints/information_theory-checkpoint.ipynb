{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34e866b",
   "metadata": {},
   "source": [
    "# 信息论基础\n",
    "## I. 基本概念\n",
    "### I.1 事件（Event）的自信息\n",
    "#### I.1.1 定义\n",
    "1. 自信息定义的数学基础\n",
    "   - 数学性质：\n",
    "     - 随机变量x的取值空间为Event space$\\mathcal{X}$\n",
    "     - 定义函数$I(P(x))$为[0,1]到信息量的映射，其中$x\\in \\mathcal{X}$\n",
    "     - 如果要满足以下三个条件\n",
    "       1. $I(P)$是P的单调递减函数\n",
    "       2. $I(P)$是P的连续函数\n",
    "       3. 当$x_1$与$x_2$相互独立时，$$I(P(x_1)*P(x_2))=I(P(x_1))+I(P(x_2))$$\n",
    "     那么I(P)的函数形式必须是:$I(P)=-Clog_b(P)$\n",
    "     - <font color=green>C和b的取值只影响函数的计量单位，但不影响不同P的函数I(P)之间的相互关系。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c220c26",
   "metadata": {},
   "source": [
    "2. **自信息的定义**\n",
    "   - 定义随机变量分布P的自信息为：\n",
    "     $$I(P)=-log_2(P)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632ed32",
   "metadata": {},
   "source": [
    "#### I.1.2 自信息的实践含义\n",
    "1. <font color=blue>假如事件x的发生概率是P(E)，那么$I(P(x))=a$就可以理解为x的概率$P(x)$的自信息量相当于**离散均匀分布的a个bit**所携带的信息量。</font>\n",
    "   - $I(P)$的定义是前述数学通式取C=1，b=2的结果。这样取值，是为了方便将$I(P)$在数值上等价于可以用bit衡量的信息大小。\n",
    "     - 说明：\n",
    "       - 1 bit可以取0，1\n",
    "       - 2 bit可以取00，01，10，11\n",
    "       - ...\n",
    "       - n bit可以取$2^n$种不同的序列，假设每种序列的取值概率相同，$P(x_i)=\\frac{1}{2^n}$\n",
    "       - 那么n bit时每种取值所含自信息量正好是n。\n",
    "       $$P(x_i)=-log_2(P(x_i))=-log_2(\\frac{1}{2^n})=n$$\n",
    "       - 含义：<font color=norange>$I(P(x))=a$意味着事件x所携带的信息量刚好等于**离散均匀分布的a个bit**所携带的信息量。</font>\n",
    "   - 可见，取C=1，b=2的优点在于，此时I(P)的值正好可以用bit来衡量，此时，自信息作为信息的计量单位，其含义非常直观。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83565ef8",
   "metadata": {},
   "source": [
    "2. <font color=blue>**如果对事件x的Event space进行编码，那么对单个事件x的最优编码长度等于其自信息的大小。**</font>(rounded up to the nearest integer)\n",
    "$$L(x)=\\left \\lceil -log_2P(x) \\right \\rceil $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109b9fd",
   "metadata": {},
   "source": [
    "### I.2 熵\n",
    "1. **熵的定义**：随机变量的自信息的期望值，衡量随机变量所携带的自信息的平均大小（有多少bit）。\n",
    "$$H(x)=-\\sum_{x\\in \\mathcal{X}}P(x)log_2P(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d58040",
   "metadata": {},
   "source": [
    "2. <font color=blue>**熵的实践含义：如果对每个event都用最优编码，那么该随机变量的平均码长等于该随机变量的熵。**</font>(简化假设不考虑round up to integer的问题)\n",
    "   - 自信息=最优编码长度，熵=平均最优编码长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918851c",
   "metadata": {},
   "source": [
    "### I.3 条件熵和联合熵\n",
    "#### I.3.1 定义\n",
    "1. 条件熵：$$H(x|y)=-\\sum_{(x|y)\\in \\mathcal{X|Y}}P(x|y)log_2P(x|y)$$\n",
    "\n",
    "2. 联合熵：$$H(x,y)=-\\sum_{(x,y)\\in \\mathcal{X,Y}}P(x,y)log_2P(x,y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d870f54e",
   "metadata": {},
   "source": [
    "#### I.3.2 性质\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62fc8ac",
   "metadata": {},
   "source": [
    "### I.4 交叉熵和KL divergence\n",
    "1. 交叉熵：\n",
    "   - 表达式：$$H(P,Q)=\\sum_x P(x)logQ(x)$$\n",
    "   - 实践含义：实际分布是$P(x)$，但却按照$x\\sim Q(x)$对分布进行编码，得到的编码的平均长度。\n",
    "2. KL divergence：\n",
    "   - 表达式：$$\\begin{align}\n",
    "D_{KL}(P||Q) & = -\\sum_x P(x)log\\frac{P(x)}{Q(x)} \\\\\n",
    "& = \\sum_x P(x)logQ(x)-\\sum_xP(x)logP(x)\\\\\n",
    "& = H(P,Q) + H(P)\n",
    "\\end{align}$$\n",
    "   - 实践含义：当x的真实分布为$P(x)$时，错误判断x的分布为$Q(x)$后得到的平均码长比按照$P(x)$编码得到的码长更长。$D_{KL}$就是这部分差异的大小，也就是错误判断分布而付出的码长代价。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
