{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2181efc6",
   "metadata": {},
   "source": [
    "# GloVe\n",
    "## I. 背景和目标\n",
    "1. 比较co-occurrence matrix和SG方法可以看出来，他们本质上使用的都是word-word co-occurrence count的信息，只是使用方法不同。但是他们各有自己的优劣势： \n",
    "   1. co-occurrence matrix用了全局的统计信息，虽然信息完整，但是得到稀疏vector。如果要用linear regularity来表达similarity，就要进一步使用COAL等方法。\n",
    "   2. SG用了local sample，对linear regularity的表达效果更好，但是没有用好全局的count信息。\n",
    "2. **GloVe的目的是：**\n",
    "   1. 既利用全局的co-occurrence count，又像SG那样用local sample做训练，用好两者的优点。所以人为定义了word vector的需要chase的语法/语义特征，将这个特征设置在目标函数中，再用local sample来learn符合这个规则的word vector。\n",
    "   2. <font color=green>Tomas Mikolov在SGNS中发现，word vector中出现的可以用加法表达的词间关系与其目标函数的设定有关。</font>沿着这个思路，GloVe提出了log biliear regularity，然后通过目标函数的设定，让训练出来的word representation能通过log biliear regularity来表达词间similarity的强弱。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fdb1cc",
   "metadata": {},
   "source": [
    "## II. 模型\n",
    "1. 符号\n",
    "   - $X_{ij}$表示context word j出现在target word i的context中的次数\n",
    "   - $X_{i}$表示word i在corpus中出现的总次数\n",
    "   - $P_{ij}=P(c_j|w_i)=X_{ij}/X_i$表示context word j出现在target word i的context中的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07f60c7",
   "metadata": {},
   "source": [
    "2. 用co-occurrence probability来表达similarity: 用概率的比值\n",
    "   - 概率本身的特点：如果target word i与context word j经常一起出现则$P_{ij}$大；如果target word k与context word j很少一起出现，那么$P_{kj}$小\n",
    "   - <font color=blue>[paper的观点]用概率的比值比用概率本身更好，可以反应出更具识别力(discriminative)的信息</font>：\n",
    "<img src=\"./pics/GloVeProbabilitiRatio.png\" style=\"zoom:70%\"> \n",
    "\n",
    "   - <font color=green>**[rk's note]如果用线性关系来表达similarity：**\n",
    "     - water + gas = stream；water + solid = ice </font>\n",
    "     - 作者认为，water和fashion对于判断ice和stream的差异而言都没有discriminative power。如果只看绝对值，那么water对于两者都高，solid和gas在两者都是中或低，fashion是都低。看不出在stream与ice的比较关系中，gas与stream更紧密，solid与ice更紧密。\n",
    "   - <font color=green>概率比值所表达的关系：\n",
    "     $$\\frac{P_{ij}}{P_{kj}}=\\frac{P(c_j|w_i)}{P(c_j|w_k)}= \\begin{cases}\n",
    "\\gg 1  & \\text{ if } j与i相关度高，与k相关度低；如ice与solid \\\\\n",
    "\\approx 1  & \\text{ if } j与i和k的相关度都高，或都低；如water和fashion\\\\\n",
    "\\ll 1 & \\text{ if } j与i相关度低，与k相关度高；如stream与solid\n",
    "\\end{cases}$$\n",
    "    可见，用概率的比值可以很好地反应word之间的similarity信息。 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456cc3b",
   "metadata": {},
   "source": [
    "3. 目标函数的设定方式 \\\n",
    "⑴ 如果用word vector来表示，那么$P_{ij},P_{kj}$是$w_i, w_k, c_j$的函数:\n",
    "$$F(w_i, w_k, c_j)=\\frac {P_{ij}}{P_{kj}}$$\n",
    "⑵ 为了表达出linear regularity，作者选择了：\n",
    "$$F((w_i-w_k), c_j)=\\frac {P_{ij}}{P_{kj}}$$\n",
    "⑶ 为了让F函数得到的是scalar，进一步具体化为：\n",
    "$$F((w_i-w_k)^T c_j)=\\frac {P_{ij}}{P_{kj}}$$\n",
    "⑷ 由于co-occurrence matrix是对称的，所以word在做target和做context的时候，其vector互换也不应该影响结果。所以，F函数还应该满足：\n",
    "$$F((w_i-w_k)^T c_j)=\\frac{F(w_i^Tc_j)}{F(w_k^Tc_j)} $$\n",
    "求解可得：\n",
    "$$w_i·c_j = logP(c_j|w_i)=log(X_{ij})-log(X_i)$$\n",
    "由此推出目标函数的基本形式：$$J=\\frac{1}{2}\\sum_{i}^{}\\sum_{j}^{} (w_i^Tc_j - logP_{ij})^2$$\n",
    "<font color=blue>此时满足关系：$$\\begin{align} \n",
    "w_i^Tc_j &  = logP(c_j|w_i) \\\\\n",
    "(w_i-w_k)^T c_j & = log P(c_j|w_i) - logP(c_j|w_k)\n",
    "\\end{align}$$作者将这种关系称为log-bilinear model </font> \\\n",
    "⑸ 为了平衡低频率次和高频率词带来的bias问题，得到最终的目标函数：$$\\begin{align} \n",
    "J & =\\frac{1}{2}\\sum_{i}^{}\\sum_{j}^{} f(X_{ij})(w_i^Tc_j +b_i+b_j- logP_{ij})^2 \\\\\n",
    "\\\\\n",
    "f(x)& = \\begin{cases}\n",
    " (x/x_{max})^{\\alpha } & \\text{ if } x<x_{max}\\\\\n",
    "1  & \\text{ otherwise } \n",
    "\\end{cases}, \\ \\ b是截距项\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a261966",
   "metadata": {},
   "source": [
    "## III. 与SG的关系：GloVe可以视为SG的变体\n",
    "1. SG的全局目标可以表示为：\n",
    "$$\\begin{align} \n",
    "Q_{ij} & =\\frac{e^{w_i·c_j}}{ {\\textstyle \\sum_{k=1}^{|V|}}e^{w_i·c_k} } \\\\\n",
    "J & = - \\sum_{i\\in corpus, j\\in context(i)}^{} log\\ Q_{ij} \\\\\n",
    "& = -\\sum_{i=1}^{|V|}\\sum_{j=1}^{|V|}X_{i,j}log\\ Q_{ij}   \\\\\n",
    "& = -\\sum_{i=1}^{|V|}X_{i}\\sum_{j=1}^{|V|}P_{ij}log\\ Q_{ij} \\ \\ ,其中P_{ij}=\\frac{X_{ij}}{X_i} \\\\\n",
    "& = \\sum_{i=1}^{|V|}X_{i}H(P_i,Q_i)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7134eedd",
   "metadata": {},
   "source": [
    "2. SG全局目标做优化可以得到GloVe的目标函数 \\\n",
    "⑴ cross entropy$H(P,Q)$是衡量两种分布的distance的measure之一。但它有两个问题： \\\n",
    "① 如果涉及的分布是长尾分布，那么用cross entropy作为目标函数对低概率事件的估计结果有很大的bias。 <font color=red>[参考文章：Seesaw Loss for Long-Tailed Instance Segmentation]</font> \\\n",
    "② $Q_{ij}$的计算复杂度高 \\\n",
    "解决方式：将目标函数中的distance measure换成least square。\n",
    "$$\\begin{align} \n",
    "J = \\sum_{i,j}^{}X_{i}(\\tilde P_{ij}-\\tilde Q_{ij})^2 \\\\\n",
    "其中，\\tilde P_{ij}=X_{ij}, \\tilde Q_{ij}=e^{w_i·c_j}\n",
    "\\end{align}$$\n",
    "⑵ 上面表达式中由于没有normalize，所以数值的数量级很大，可以用log来降低数量级，因此，进一步更改为：$$\\begin{align} \n",
    "J & = \\sum_{i,j}^{}X_{i}(log\\tilde P_{ij}-log\\tilde Q_{ij})^2 \\\\\n",
    "& = \\sum_{i,j}^{}X_{i}(w_i·c_j-logX_{ij})^2\n",
    "\\end{align}$$\n",
    "⑶ 还有一个问题，就是高频词汇的信息量少，要特殊处理。\\\n",
    "在SGNS中Tomas Mikolov用了修正后的均匀抽样。而这里特别高频的词汇会体现在$X_i$的数值很大。简单的处理方式是直接将$X_i$改为$f(X_i)$。就得到GloVe的目标函数：\n",
    "$$J = \\sum_{i,j}^{}f(X_{i})(w_i·c_j-logX_{ij})^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c27c5c",
   "metadata": {},
   "source": [
    "3. <font color=red>[rk note] 注意：从上面的理论逻辑上看，GloVe是对SG的一些问题做了优化，但是SGNS由于改变了目标函数，所以同样的问题SGNS并没有，或者也用了别的方式解决，所以理论上GoVe没有证明相对SGNS的优点。从实际效果上看SGNS更好。</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224nhw",
   "language": "python",
   "name": "cs224nhw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
