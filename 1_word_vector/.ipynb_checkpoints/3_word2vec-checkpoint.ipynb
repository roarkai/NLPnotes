{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc274c71",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "## I. 背景、要解决的问题和思路\n",
    "### I.1 背景\n",
    "- 当时有三种word representation的方法。Bengio等团队已经证明在相同训练数据量的情况下，word embedding用在下游任务上比count-base的word representation效果好。而且training set越大，效果越好。\n",
    "  - one-hot vector(sparse)\n",
    "  - count-based co-occurrence matrix (sparse和dense都有)\n",
    "  - word embedding(dense)\n",
    "- 但是word embedding是NN model的副产品，训练NN的成本高。\n",
    "  - 如果NN处理的是MT或者speech recognition等supervised learning任务，还有语料少的问题。\n",
    "  - 如果是LM，因为用unsupervised learning，可以不受语料限制，但是计算成本仍然很高。在word2vec出来之前，在公开的研究中，没有基于NN的LM处理过Billion级的数据。当时的默认策略是用n-gram LM加billion级数据。靠n-gram这种简单模型加大量数据的效果通常比复杂的NN model加少量(million级)数据效果更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12630360",
   "metadata": {},
   "source": [
    "####  I.1.1 word representation\n",
    "  1. 2013年之前主要有两类learn word vector的方法。\n",
    "     - 第一种是**distributional representation**：手工做feature engineering，基于co-occurrence matrix来计算word vector。典型代表是tf-idf、PPMI、LSA、LDA。<font color=blue>得到sparse vector.</font>\n",
    "     - 第二种**distributed representation**：其核心思想是grouping similar words，让相似(经常在句子中一起出现)的words有相似的vector。\n",
    "       - 常用的生成方式是：在NN的language model任务中用one hot vector做为input，把第一层的weight matrix作为word vector。<font color=blue>得到dense vector.</font>\n",
    "   2. distributed representation在下游任务上的计算效率更高，而且效果也更好\n",
    "      - 计算效率方面：用NN learn得到的dense feature通常只有50-200维，而sparse vector一般都在5000维以上。在下游任务中作为input的话，能大幅减少NN要learn的weights数量。\n",
    "      - 效果方面：\n",
    "        1. 由于要learn的参数少，因此overfit也更少。特别是早期任务很多都是supervised learning，training set的大小有限。\n",
    "        2. 在learn word similarity和analogy关系上，特别是analogy关系上，distributed representation的效果更好。<font color=brown>[详见后文word2vec和其他方法的比较]</font>\n",
    "      - <font color=blue>**distributed representation在当时主要是NN model的副产品**</font>\n",
    "        - MLP和RNN最终learn到的model中，第一层的参数就是word的distributed representation，又称为word embedding.单独把这部分拿出来用到其他NLP下游模型上，通常能帮助下游模型取得更好的效果。所以当时也出现了很多关于用NN来learn word embedding的研究，目标是找到计算成本低，并且能很好表达word relationship的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d768c35",
   "metadata": {},
   "source": [
    "####  I.1.2 language model\n",
    "- 最早把NN用到LM上的是Yoshua Bengio团队，他们在2000年投稿NIPS的第一版《A Neural Probabilistic Language Model》论文中就把MLP结构的NN用到LM上，并且展示了明显好于传统n-gram model的效果。但是接下来的十年中，NN都不是LM的主流模型。主要原因是NN的计算成本比n-gram model高很多。主要的计算开销用在training和最后一层用softmax计算probability上。\n",
    "  - Bengio团队提出的neural probabilistic LM结构图：\n",
    "    <img src=\"../pics/bengioNNforLM.png\" style=\"zoom:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25694e7b",
   "metadata": {},
   "source": [
    "- 当时用million级的数据训练一个MLP model来解决NLP任务通常需要几天到几周。到2013年还没有NN模型用billions级的语料来做训练。而且MLP很难处理长句，本身不适合于language中涉及的long sequence问题。\n",
    "- 2010年后RNN逐渐被用到LM上，RNN更适合处理long sequence问题。Mikolov团队在《recurrent neural network based language model》中把RNN用到LM上解决语音识别任务。但RNN有比MLP和CNN都更严重的梯度消失和梯度爆炸问题，难以训练。Graves团队在2013年的时候用了LSTM来处理RNN训练中梯度消失和爆炸导致的memory lose的问题。使得RNN训练难度有所降低，但仍然不容易。\n",
    "- 尽管已经在效果上取得了明显的进展，但直到2013年，因为计算成本的限制和训练难度的问题，基于NN的方法都没有被接受为LM问题的标准处理方式。\n",
    "- 另外，如果处理的不是LM问题，那模型就是supervised learning，此时还要受到可用语料数量的限制。\n",
    "- 而训练基于n-gram的LM则简单的多。n-gram也是self-supervised model，所以可用语料多。同时算法也简单，learning速度可以很快。尽管同样样本量的条件下效果明显不如基于NN的方法，但由于简单，可以使用更大数量的样本来获得更好的效果，所以到了2013年，LM类任务的默认选择还是n-gram。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd414d4e",
   "metadata": {},
   "source": [
    "#### I.1.3 附加信息，早期GPU的使用情况\n",
    "- 2009年吴恩达团队trained a 100M deep belief network on 30 Nvidia GeForce GTX 280 GPUs, demonstrating the use of GPUs for deep learning.\n",
    "- 2011年吴恩达到google，和Jeff Dean, Greg Corrado和Rajat Monga一起建立了Google Brain。\n",
    "- 2012年alex在两张NVIDIA GTX 580 3GB GPUs上用5、6天时间训练了Alexnet.\n",
    "- 2013年google启动了TPU项目，2015年第一批TPU开始内部试用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e399c897",
   "metadata": {},
   "source": [
    "### I.2 要解决的问题\n",
    "  - 找到一种计算效率高的方法，能够处理大量语料，并且learn到高质量的distributed word representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd1c57",
   "metadata": {},
   "source": [
    "### I.3 思路和创新点\n",
    "#### I.3.1 思路\n",
    "为了找到一种简单的model结构，用很少的算力处理billion级的文本数据。作者的思考体现在下面几点：\n",
    "1. <font color=blue>首先model应该是LM。这样才能用unsupervised learning，才能有billion级数据可用。这意味着目标函数应该计算P(word|context)。</font>\n",
    "2. <font color=blue>目标函数要尽可能表达word关系中的linear regularity。</font>作者在之前的一篇论文中已经观察到word vector能以线性关系来表达semantic regularities和syntactic regularities。从而提出word vector evaluation用linear regularity标准。这篇文章沿着这个思路，在设计model结构的时候，用bi-linearity来处理center word和context word之间的similarity，通过这种方式直接把linearity加到目标函数上，让模型learn到的linear regularity更好。\n",
    "3. <font color=blue>模型结构要简单，算法复杂度低。</font>分析MLP和RNN的算法复杂度，作者发现：大量的计算集中在hidden layer上。但这是NN为了表达力而付出的代价。但本文作者认为，舍弃模型的表达力，提高计算效率，从而能处理远超以往的数据在word representation这个任务上可能是可行的。<font color=brown>**[rk's note]**:这点应该跟上一条有关，如果只是为了得到word representation，不完成具体的NLP任务，那么线性关系反而可能让vector中表达更多linear regularity。那么用多层结构来增加non-linearity的表达能力这个需求也会小一些，就可以去掉hidden layer，直接把矩阵乘法得到的线性值交给softmax函数去处理。这样处理可以节省大量运算。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8608728",
   "metadata": {},
   "source": [
    "#### I.3.2 方法和效果\n",
    "- **方法**\n",
    "1. 用unsupervised learning，所以可以不受当时困扰多数NLP任务的标记语料数量的限制。\n",
    "2. 模型结构简单，用bi-linearity来处理center word和context word之间的similarity，把结果直接softmax。\n",
    "3. 为了减少softmax带来的大量计算，改用logistic function加negative sampling来近似softmax。\n",
    "- **效果**\n",
    "1. 涉及的矩阵乘法运算量小，因此计算效率比过去见到的NN高很多。在大幅降低计算量之后，实现了使用Billion级的语料来做训练。这也是使得到的word vector效果好的一个主要原因。\n",
    "2. 不仅learn到了semantic regularities和syntactic regularities还learn 到了另一种加法线性关系：$v_z \\approx v_x+v_y$。比如：China+currency接近RMB。\n",
    "3. 由于使用了超过以往任何NN结构的模型的数据量，并且目标函数的设计方式更容易找到word关系中的linear regularity，而evaluation本身又用的是linear regularity，word2vec成了那一时期word representation model的SOTA。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0432ea",
   "metadata": {},
   "source": [
    "#### I.3.3 和当时主流的其他方法的比较\n",
    "- <font color=brown>参考：《improving distributional similarity with lessons learned from word embedding》</font>\n",
    "  1. 数值特征的差异：word2vec得到的vector是dense的，并且vector的每个元素不再像co-occurrence matrix中那样有明确的含义。\n",
    "  2. 实践效果：word2vec在实际任务上的效果往往比co-occurrence matrix中vector的效果好。原因：\n",
    "     - 因为维度少，用它们作为网络的input时，要learn的weights就会更少，因此overfit就少。\n",
    "     - wrod2ec 比co-occurrence matrix能更好地表达近义关系。比如：在word-word vector中，column中的‘car’和‘automobile’对应不同的维度，他们在数值上看，是两个正交的方向，两个vector的context中分别有car和automobile的话，这两个信息无法交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc4314",
   "metadata": {},
   "source": [
    "## II. 系列模型的设计\n",
    "### II.1 word2vec的基本思路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ee178",
   "metadata": {},
   "source": [
    "#### II.1.1 系列模型的特点：\n",
    "1. <font color=blue>**intuition**：</font>用inner product衡量relatedness或者similarity，如果两个word的relatedness越高，那么他们互相在彼此的上下文中的概率就越大。$$Similarity(w,c)\\approx \\left \\langle c,w \\right \\rangle$$\n",
    "2. <font color=blue>**思路**：</font>\n",
    "   1. 表达intuition的方式：用logistic或者sigmoid function，将similarity与probability联系起来。\n",
    "      - 如果用logistic\n",
    "      $$P(c_i|w)=\\sigma (\\left \\langle c_i,w \\right \\rangle)=\\frac{1}{1+e^{-\\left \\langle c_i,w \\right \\rangle}}$$\n",
    "      - 如果用sigmoid \n",
    "      $$P(c_i|w)=\\frac{e^{c_i·w}}{ {\\textstyle \\sum_{c_j\\in |v|}^{}}e^{c_j·w} }$$\n",
    "   2. <font color=norange>训练一个classifier来判断word是不是target word的context word。并且为了实现计算的高效率，去掉NN中的hidden layer，直接把word vector用来计算probability。</font>\n",
    "   3. 将classifier training得到的parameters作为target word和context word的word vector。\n",
    "3. 符号\n",
    "   - vocabulary的长度为N，corpus的长度为T\n",
    "   - context window长度为L\n",
    "   - 1个positive sample(target word and context word pair)对应的negative sample(target word and non-context word pair)数量为k \n",
    "   - $P(c|w)$表示word $c$是word w的context word的概率\n",
    "   - $P(c_{1:L}|w)$表示$c_{1:L}$都是word w的context words的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184bfd16",
   "metadata": {},
   "source": [
    "#### II.1.2 论文中用了不同的算法：\n",
    "- classifier用的算法：CBOW（用context word估计sample word是不是center word），Skip-Gram（用center word估计sample word是不是center word）\n",
    "- 抽样方法：naive sampling, Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a2f15f",
   "metadata": {},
   "source": [
    "### II.2 naive skip-gram (SG)\n",
    "1. 思路：\n",
    "   - i. 给一个target word，用它计算所选窗口中context word发生的概率是P(c|w)。假设context word一旦出现在window中，那么具体在哪个位置并不重要，不管出现在window中哪个位置，使用的概率都相同，即$P(c_i|w)=P(c_j|w),i,j$是position index。<font color=green>这一点和Bag of words的思想一致。</font>\n",
    "   - ii. 用softmax function来表达$P(c_i|w)$\n",
    "   $$P(c_i|w)=\\frac{e^{c_i·w}}{ {\\textstyle \\sum_{c_k\\in |v|}^{}}e^{c_k·w} }$$\n",
    "   - iii.context word相互独立，因此所有L个context word一起出现的概率：$$P(c_{1:L}|w) = {\\textstyle \\prod_{i=1}^{L}} P(c_i|w)$$\n",
    "       - <font color=red>这个设定也导致SG学习到的word representation无法表达order，比如：习语短语如Boston Dynamic，无法与Dynamic Boston做区分，而且本身表达的含义不应该是\"Boston + Dynamic\"。</font> \n",
    "   - iv. 为简化求解过程，让每个word作为target word和context word有不同的vector。 \n",
    "   - v. 目标是最大化语料库中样本window中target、context word样本发生的概率：$$\n",
    "Loss = \\frac{1}{T} {\\textstyle \\sum_{t=1}^{T}}  {\\textstyle \\sum_{j=1}^{L}}logP(c_j|w_t) \n",
    "$$\n",
    "2. 特点：\n",
    "   - i. 模型简单运算效率高，加上self-supervised learning可以利用大量的预料，因此把实践中word vector训练的预料规模和vector的长度都大幅提升。\n",
    "   - ii. 不足是学习到的word representation无法表达order\n",
    "   - iii. P(c|w)的计算用softmax需要的计算量太大，实际应用中用hierarchical softmax。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89de34",
   "metadata": {},
   "source": [
    "### II.3 skip-gram with negative sampling (SGNS)\n",
    "#### II.3.1 在naive SG基础上做了3点改进\n",
    "   1. 用negative sampling，优点是比naive SG中用的hierarchical softmax有更高的训练效率，更好的高频词汇的表达准确度\n",
    "      - **[rk's note]** \n",
    "        1. <font color=red>SGNS为了减少运算复杂度，改用logistic，也因此要配合用negative sample</font> \n",
    "        2. <font color=green>如果用sigmoid的话，不需要negative sample。因为sigmoid中计算P(c|w)的方式已经包含了如果要最大化$P(c_i|w)$那么其他非$c_i$的context的概率就会降低，sigmoid中normalize的方式已经暗含了这个约束。但是logistic中没有。</font>\n",
    "   2. 用sub-sampling of frequent words，优点是提高训练效率，增加低频词汇的表达准确度\n",
    "   3. 对idiomatic phrases(组织名称、产品品牌等专有词组/短语)做专门的训练，对每个词组训练phrase vector，而不是直接用它们包含的words的vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e6e0cd",
   "metadata": {},
   "source": [
    "#### II.3.2 模型设定：\n",
    "- 假设i,iii,iv,v和naive SG的设定一样，只有假设ii做了改变：不再用softmax function而是用logistic function来计算$P(c|w)$.\n",
    "     $$\\begin{align} \n",
    "P(c_{1:L}|w) & = {\\textstyle \\prod_{i=1}^{L}} P(c_i|w) \\\\\n",
    "& = {\\textstyle \\prod_{i=1}^{L}} \\sigma (\\left \\langle c_i,w \\right \\rangle) \\\\\n",
    "& ={\\textstyle \\prod_{i=1}^{L}}\\frac{1}{1+e^{-\\left \\langle c_i,w \\right \\rangle}} \\\\\n",
    "\\end{align}$$ \n",
    "<font color=blue>虽然这个独立假设和SG中一样，会导致无法表达短语，但SGNS paper中用了phrase vector来解决短语识别问题。</font><font color=red>SGNS并没有从根本上解决习语的表达的问题，只是单独为习语短语(idiomatic phrase)如\"New York\"做了训练。使得\"New York\"有自己的vector，而不是用\"New\"和\"York\"的vector来表达。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4dae4b",
   "metadata": {},
   "source": [
    "#### II.3.3 learning algorithm\n",
    "1. 抽样：\n",
    "   - 从语料中抽target word，得到长度为L的窗口内的L个context word，构成L个positive sample。\n",
    "   - 一组样本包含1个positive sample和与之匹配的k个negative sample。negative sample是用target word在training set中的unigram distribution抽样得到的。注意抽样使用的概率不是raw frequency，而是weighted unigram frequency。[见下面说明] \n",
    "2. 整体目标是：\n",
    "   $$ max \\frac{1}{T}{\\textstyle \\prod_{t=1}^{T}}  {\\textstyle \\prod_{i=1}^{L}}P(c_{pos_i}|w_t) {\\textstyle \\prod_{j=1}^{k}}(1-P(c_{neg_j}|w_t))$$\n",
    "3. 用SGD做迭代：\n",
    "   - 一次迭代（对应一组样本）的最大似然估计：\n",
    "     $$\\begin{align} \n",
    "&\\underset{w, c}{argmax} P(c_{pos}|w) {\\textstyle \\prod_{i=1}^{k}} (1-P(c_{neg_i}|w))\\\\\n",
    " =& argmax logP(c_{pos}|w) +  {\\textstyle \\sum_{i=1}^{k}} log\\left (1-P(c_{neg_i}|w)  \\right ) \\\\\n",
    " =& argmax log\\sigma (c_{pos}·w) + {\\textstyle \\sum_{i=1}^{k}} log \\sigma (-c_{neg_i}·w ) \\end{align}\n",
    "$$\n",
    "   - 梯度：$$\\begin{align} \n",
    "\\frac{\\partial l}{\\partial c_{pos}} & =(1-\\sigma (c_{pos}·w))w \\\\\n",
    "\\frac{\\partial l}{\\partial c_{neg_i}} & =-\\sigma (c_{neg_i}·w)w \\\\\n",
    "\\frac{\\partial l}{\\partial w} & =(1-\\sigma (c_{pos}·w))c_{pos}- {\\textstyle \\sum_{i=1}^{k}}\\sigma (c_{neg_i}·w)c_{neg_i}  \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1f38f",
   "metadata": {},
   "source": [
    "#### II.3.4 抽样方法\n",
    "1. negative sampling\n",
    "   - 方法：negative sample的抽样不是按照word的frequency来做抽样，而是要加权重，用weighted unigram frequency。\n",
    "$$\\begin{align} \n",
    "P_\\alpha(w) & = U(w)^{\\alpha }/Z \\\\\n",
    "& = \\frac{count(w)^\\alpha}{ {\\textstyle \\sum_{w^{'}}^{}}count(w^{'})^\\alpha  \n",
    "},\\ 经验取\\alpha=\\frac{3}{4} \\end{align}$$\n",
    "   - 原因：<font color=blue>主要是为了让低频词被抽中的概率高一些，高频词的低一些。因为**词语在预料中的分布是长尾分布。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5ac1a",
   "metadata": {},
   "source": [
    "2. subsampling of frequent words：也是为了平衡高频词和低频词的概率。但是效果很小，设定也很主观，后来的模型中这个用得少。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050fcdd7",
   "metadata": {},
   "source": [
    "3. <font color=green>**应用中的tricks**</font> \\\n",
    "① 参数设置：如果预料库很大，window的长度可以是2-5；如果语料库不大，长度可以设为5-20. \\\n",
    "② 因为learn到的结果里面每个word有两个vector，在具体使用的时候，通常是取两个vector的均值，或者删除作为contect word得到的context vector，只用target vector。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d523a",
   "metadata": {},
   "source": [
    "#### II.3.5 效果和说明\n",
    "- 符号说明：这里用$\\approx$表示“距离”近，用公式表示为：$$w^*=\\underset{w}{argmax}\\frac{v_{target\\,posi}^Tw}{\\left \\| v_{target\\,posi} \\right \\| \\left \\| w \\right \\|} $$\n",
    "  - target position是用analogy关系计算出来的精确位置，比如：$v_{target\\,posi}可以取w_{king}-w_{man}+w_{woman}$\n",
    "  - $w^*$是所有word中用cosine计算的与target position距离最近的那个vector。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37432460",
   "metadata": {},
   "source": [
    "1. 效果：在当时的syntactic和semantic linear regularity中都达到了SOTA.得到的word vector表达了两类linear structure：\n",
    "   1. 如果两组word之间有syntactic和semantic analogy关系：$b + (a^*-a) \\approx b^*$\n",
    "   2. 如果x,y经常出现在z的context中，SGNS会得到$v_z \\approx v_x+v_y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4958c28f",
   "metadata": {},
   "source": [
    "2. <font color=norange>**说明：learn到的word vector中的线性关系与目标函数设定中使用的$w$与$c_i$的双向线性关系(bi-linearity)有关。**</font>\n",
    "   - 论文中的解释：为什么当x,y经常出现在z的context中，SG会得到$v_z \\approx v_x+v_y$，比如：$China+currency \\approx RMB$\n",
    "     - 根据softmax版本的目标函数:\n",
    "     $$\\begin{align}\n",
    " logP(c_x|w_z) &=c_x^Tw_z + log\\Sigma_{k=1}^{|V|} e^{c_k^Tw_z} \\\\\n",
    " logP(c_y|w_z) &=c_y^Tw_z + log\\Sigma_{k=1}^{|V|} e^{c_k^Tw_z} \\\\\n",
    "\\therefore  logP(c_x|w_z)P(c_y|w_z) & = (c_x^T+c_y^T)w_z + 2log\\Sigma_{k=1}^{|V|} e^{c_k^Tw_z}\n",
    "\\end{align}$$\n",
    "     - 从上式可知，如果x，y经常出现在z的context中，那么$P(c_x|w_z)*P(c_y|w_z)$应该尽可能高。而为了满足这个特征，右边的$(c_x^T+c_y^T)$会接近$w_z$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315701f",
   "metadata": {},
   "source": [
    "- <font color=green>**[rk's note]**</font>：<font color=deeppink>**为什么SG的目标函数会learn到$b + (a^*-a) \\approx b^*$**</font>\n",
    "  - 比如：$king - man + woman \\approx queen$\n",
    "    - 实际上这个关系可能衍生于另外两组关系：$$\\begin{align}\n",
    "\\because \\begin{cases}\n",
    "king - he + she \\approx queen \\\\\n",
    "man - he + she \\approx woman\n",
    "\\end{cases} \\\\\n",
    "\\therefore king - man + woman \\approx queen\n",
    "\\end{align}$$\n",
    "    - 两组关系在语法上可以找到线索：\n",
    "      - king出现在he的context中的概率应该接近于queen出现在she的context中的概率，也就是$P(King|he)\\approx P(Queen|she)$\n",
    "      - 类似有，man出现在he的context中的概率也应该接近于woman出现在she的context中的概率，$P(man|he)\\approx P(woman|she)$\n",
    "      - 在SG模型的目标函数中，上述关系表示为：\n",
    "\n",
    "$$\\begin{align}\n",
    "&logP(King|he)\\approx logP(Queen|she) \\\\\n",
    "\\rightarrow & C_{King}^TW_{he} + log\\Sigma_{k=1}^{|V|}e^{C_k^TW_{he}} \\approx C_{Queen}^TW_{she} + log\\Sigma_{k=1}^{|V|}e^{C_k^TW_{she}} \\\\\n",
    "&logP(man|he)\\approx logP(woman|she) \\\\\n",
    "\\rightarrow & C_{man}^TW_{he} + log\\Sigma_{k=1}^{|V|}e^{C_k^TW_{he}} \\approx C_{woman}^TW_{she} + log\\Sigma_{k=1}^{|V|}e^{C_k^TW_{she}} \\\\\n",
    "\\therefore & (C_{King}^T-C_{man}^T)W_{he}\\approx (C_{Queen}^T-C_{woman}^T)W_{she} \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d382b2",
   "metadata": {},
   "source": [
    "- <font color=green>**[rk's note]**</font>：如何理解$(C_{King}^T-C_{man}^T)W_{he}\\approx (C_{Queen}^T-C_{woman}^T)W_{she}$\n",
    "  1. <font color=blue>he和she的word vector有很明确的关系</font>：\n",
    "     - 理想状况下，如果latent feature学的较好，就会有一个dim对应的是gentle属性，有一个对应object feature,具体值体现human。she和he的word vector上的gentle属性取值高但符号相反，object/human取值高且接近，其他dim取值小且接近。\n",
    "     - 在SG中，这种关系应该可以通过he和she周围的context word的distribution被learn到。\n",
    "  2. 从数值上看，如果$(C_{King}^T-C_{man}^T)W_{he}\\approx (C_{Queen}^T-C_{woman}^T)W_{she}$，说明在gentle、object/human这两个$he$和$she$有较高值的feature位置上$C_{King}^T-C_{man}^T$与$W_{he}$的match程度和$(C_{Queen}^T-C_{woman}^T)与W_{she}$的match程度接近。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28fca6",
   "metadata": {},
   "source": [
    "3. <font color=green>[rk's note]</font>**GloVe是上述思路的扩展。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224nhw",
   "language": "python",
   "name": "cs224nhw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
