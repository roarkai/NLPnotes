{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab85e89",
   "metadata": {},
   "source": [
    "# Representation Learning Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd8cde",
   "metadata": {},
   "source": [
    "## I. 理解word meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3ccf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T01:53:33.807363Z",
     "start_time": "2023-12-15T01:53:33.795578Z"
    }
   },
   "source": [
    "1. model of word meaning: 一个好的model of word meaning应该可以较好的表达词汇的含义在语义空间的分布。比如能够表达：\n",
    "   1. 近义词、反义词。 \n",
    "   2. 词语的感情色彩：褒义和贬义 \n",
    "   3. 同属于一簇的词汇（如：buy，sell，pay都是购物场景的词汇；cat，dog都是动物）\n",
    "- 更一般地说，一个好的model of word meaning应该有助于解决meaning-related task。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba7119",
   "metadata": {},
   "source": [
    "2. lexical semantics（词汇语义） \n",
    "   - lemma和word form：lemma是词元、词根，word form是用词根生成的具体的词形。每个lemma有多种含义（word sense）\n",
    "   - **synonyms同义词**: \n",
    "     1. synonym between word senses：指1个word的1种word sense与另一个word的1种word sense相同。这是可能发生的。\n",
    "     2. synonym between words：指两个word含义相同可以互相替代。<font color=red>但实际上没有真正的synonym between words，water和$H_2O$指同样的东西，但使用场景不同。</font> \n",
    "   - **silimarity**:\n",
    "<font color=green>**在representation learning中通常关注的是word similarity**：在word sense的层面建模的话，同一个word有不同的representation，实际任务中很难处理。如果改在word层面建模，因为没有真正的word synonym，关注synonym没有意义，所以关注word similarity是更符合word真实情况的方式。</font>\n",
    "   - **word relatedness**: 词与词之间的关系不止similarity。还有很多其他关系，如：\n",
    "     1. sematic field：一种典型的词之间的relatedness是<font color=blue>**词语有相同的sematic field。**</font>比如：waiter,menu,plate,food, chef都属于sematic field of restaurant，而且这些词本身的词义没有similarity。\n",
    "     2. sematic frame：一组含有特定事件（event）信息的词，不同的词在事件中的角色不同。比如，以transaction这个event为例，会有seller, buyer, pay, money, goods等角色，而不同的words会承担这些角色。如果representation可以表达出这部分内容的话，就能识别出Bob bought a desk from Jim. Jim sold his desk to Bob.这两句话含义相同。\n",
    "     3. connotation: 语言中的情绪，包括：emotion, sentiment, evaluation, or opinion. \n",
    "     4. 其他，反义词、上义词、下义词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9d1ae",
   "metadata": {},
   "source": [
    "3. vector sematics\n",
    "   - 含义：指的是用vector表示word，每个word是多维语义空间中的一个点。这个语义空间是利用语料中的word disrtibution信息得到的。 \n",
    "   - 优点：可以用无监督学习自动从语料中获得vector sematic \n",
    "   - 方法：早期的方法有tf-idf和ppmi，后面NN learning和word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b29fa",
   "metadata": {},
   "source": [
    "## II. word vector在语义(sematics)方面的性质\n",
    "- <font color=green>注：这部分内容是SLP书中主要归纳了用context window信息来训练的model所得到的embedding的性质</font>\n",
    "\n",
    "1. context window长度设定会让embeddings获得三种不同的similarity or association：\n",
    "一般k的长度会选两边各1-10个words，k=2~20。 \n",
    "   - ① 如果k较小，就是周围2、3个位置的词，得到的word representation中带有的syntactic（句法的）信息更多 \n",
    "   - ② 如果k虽然还是小，但不是只看周围2、3个位置，那么这些位置的context word与target word的semantic similarity相对大。 \n",
    "   - ③ 如果k较大，那么positive sample中的context word与target word的关系是topically related，但similarity不见得大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00927a86",
   "metadata": {},
   "source": [
    "2. 区分两种similarity or relatedness \\\n",
    "① first-order co-occurrence, 又称为syntagmatic association（组合相关性），表示两个词通常出现在彼此的旁边。比如wrote和book，ride和bike。 \\\n",
    "② second-order co-occurrence, 又称为paradigmatic association（排比关系），表示两个词有相同的邻近词。比如wrote，said和remarked。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697db0f",
   "metadata": {},
   "source": [
    "3. embeddings表达analogy meaning的能力: \n",
    "   - **analogy meaning**：a is to b as $a^{'}$ is to what? \n",
    "     - 比如: $apple:tree::grape:\\_?$ 答案是vine.  \n",
    "   - 用word vector来对类比关系进行数学化处理的方法是parallelogram method：\n",
    "     - 先定义distance function，再求解：$$b^{'} = \\underset{x}{argmin}\\ distance(x, b-a+a^{'})$$\n",
    "   - 实践结果：word2vec和Glove这些方法在特定的analogy problem上效果很好。但通常是distance较小，涉及的word是高频词汇，涉及的词间关系也是特定关系如（性别职业、国家首都、动词的名词对应等），超过这些特定范围的效果就不太好。\n",
    "   - <font color=green>有的研究者认为，问题在于使用$b^{'} = \\underset{x}{argmin}\\ distance(x, b-a+a^{'})$ 的parallelogram method过于简化，不能反映出word之间的复杂关系。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45af9a7",
   "metadata": {},
   "source": [
    "4. sematic meaning的变化  \\\n",
    "现实中，词语的sematic meaning在长期来看是不断变化的。如果使用不同时代的语料来得到word embedding，得到的结果会不一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64362b2e",
   "metadata": {},
   "source": [
    "## III. word representation model的evaluation\n",
    "### III.1 vector model评价标准的类型\n",
    "   1. extrinsic evaluation：一般来说最好的评价方式就是看在具体任务上的结果 \n",
    "   2. intrinsic evaluation：2013年前后最常用的方式是test the performance on similarity。比较人对词语相似性的评估结果和embeding的评估结果的差异。这里主要介绍2013年前后所使用的intrinsic标准。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440dca1c",
   "metadata": {},
   "source": [
    "### III.2 SG系列model提出的评价标准\n",
    "- SG论文中提出了word vector所表达的词语间的线性关系（如下文）。使得多大程度上能更好的表达这种linear structure成为了评价word representation结果的标准。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4a61b",
   "metadata": {},
   "source": [
    "- <font color=blue>**线性关系的具体类型：类比关系和加法关系**</font>\n",
    "  1. 类比关系：a - b + c = d\n",
    "     - 语法上的类比关系 \n",
    "       - 例如：\n",
    "         - bigger - big + high = higher； \n",
    "         - think - thinking + read = reading \n",
    "     - 语义上的类比关系 \n",
    "       - 例如： \n",
    "         - Athens - greece + Oslo = Norway；\n",
    "         - brother - sister + granddaughter = grandson \n",
    "  2. 加法关系: a + b = c\n",
    "     - 加法关系是语义上的，比如：\n",
    "       - china + currency = yuan；\n",
    "       - Russian + river = Volga river"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a29cdc8",
   "metadata": {},
   "source": [
    "- 注意: 这并不是衡量所有模型的word vector的唯一标准。因为，虽然这个标准对词语之间的关系给出了数学上的线性关系的模型(a - b + c = d和a + b = c)但是这两个数学关系本身是在使用了线性模型而得到的vector中总结出来的。<font color=red>**得到的vector中存在的线性数学关系和使用的目标函数有关。**</font><font color=green>[详见SGNS paper打印版上的rk's note:证明了加法关系就是由SGNS的目标函数导致的]</font> \\\n",
    "换句话说，如果用了非线性模型来得到word vector，结果可能不能较好地用linear regularity来衡量。\n",
    "- 终极标准还是看vector在具体NLP任务上的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902bf36f",
   "metadata": {},
   "source": [
    "### III.3 PPMI、SVG、SGNS、GloVe四种模型比较\n",
    "<font color=brown>[**参考论文**：improving distributional similarity with lessons learned from word embeddings]</font> \\\n",
    "<font color=green>[rk's note] **这篇文章给超参数的设定提供了参考。**</font>\n",
    "<font color=red>**这里用intrinsic evaluation**</font>\n",
    "\n",
    "1. 比较对象（2种类型4种方法）：<font color=blue>**PPMI、SVG、SGNS、GloVe**</font> \\\n",
    "① **count-based representation**：用co-occurrence count来得到vector，如：PPMI、SVG。 \\\n",
    "② **pediction-based representation**：用context word预测target word，或者用target word预测context word。如：SGNS、GloVe，另外还有SG、CBOW。\n",
    "2. 2类比较方法 \\\n",
    "① google和msa都出了评价analogy detection的数据集 \\\n",
    "② 还有一些评价similarity的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9693b5a",
   "metadata": {},
   "source": [
    "3. **结论** \\\n",
    "prediction-based representation没有比count-based representation明显更好。四种模型的参数设定对结果有较大影响，如果各自都用好超参数，那么他们在不同的任务中各有优劣。如下图： \\\n",
    "<img src=\"../pics/word_representation_result_1.png\" style=\"zoom:70%\"> \\\n",
    "总的来说也有一些局部结论：\\\n",
    "⑴ SGNS大多数情况下都比GloVe好 \\\n",
    "⑵ 在analogy关系的任务中，prediction-based representation表现更好 \\\n",
    "⑶ 总的来说，虽然没有某个绝对好的方法，但SGNS相对提供了较好的baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5ee69",
   "metadata": {},
   "source": [
    "4. **对结论的理解** \\\n",
    "<font color=blue>⑴ 从两种方法所利用的信息源来看，count-based representation和prediction-based representation本质上使用的都是每个word的context中所含word的信息。</font> \\\n",
    "<font color=brown>**这些模型本质上都是\"bag of words methods\"。指每个word representation都是a weighted bag of context-words that co-occur with it.**</font> \\\n",
    "<font color=blue>⑵ 从两种方法寻找vector所使用的数学工具来看，如果给PMI、SCD、SGNS和GloVe加上各自的超参数，可以找到他们之间的近似等价形式。</font> \\\n",
    "<font color=green>由于他们从数学方法到信息源都有一致性，因此，在结果上表现出一致性也就合理。</font> \\\n",
    "实际任务中还是会有差异： \\\n",
    "① 算法复杂度不同导致训练效率不同。\\\n",
    "② sparsity也会影响模型效率。 \\\n",
    "③ 不同的任务本身可能对不同的模型能力有不同需求，效果还是有一些差异。比如，有的任务可能需要很强的analogy关系的识别能力，此时SGNS是最好的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e6513",
   "metadata": {},
   "source": [
    "## IV. word embedding的可视化\n",
    "方法：\n",
    "1. hierachical clustering\n",
    "2. 线性降维：PCA\n",
    "3. 非线性降维：t-SNE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224nhw",
   "language": "python",
   "name": "cs224nhw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
