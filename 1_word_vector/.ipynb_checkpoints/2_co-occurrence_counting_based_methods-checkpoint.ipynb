{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220abefe",
   "metadata": {},
   "source": [
    "# co-occurrence counting based methods\n",
    "- 这是早期手工计算word vector的方法，相当于做feature engineering\n",
    "## I. term-document matrix\n",
    "1. 结构: 每行表示词汇表中的1个word，每列对应corpus中的1个document，table中第i行j列的value是：$$v_{i,j}=\\# word_i\\ in\\ doc_j$$\n",
    "   - 表示word i在document j中出现的次数。\n",
    "   - word对应的row value作为该word的vector。\n",
    "     <img src=\"../pics/term_document2.png\" style=\"zoom:100%\">\n",
    "   - document对应的column value作为该doc的vector。\n",
    "     <img src=\"../pics/term_document.png\" style=\"zoom:100%\">\n",
    "2. intuition： \\\n",
    "① 用document中包含的word信息来表达document。每个doc用维度为|V|的向量表示，|V|是词典长度。所以每个doc对应|V|维空间中的一个点。\\\n",
    "② <font color=green>**term-document matrix最初是为信息检索而设计的。**</font>如果两个doc中出现的word相似，那么这两个doc的内容会更相似。他们在|V|维空间中的距离也更近，所以可以用这个vector来做信息检索。\\\n",
    "③ 用word出现的document信息来表达word meaning。 \\\n",
    "④ <font color=red>如果两个word出现在类似的doc中，说明这两个word相似度高。</font>\n",
    "3. vector的数值特点：sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b7f129",
   "metadata": {},
   "source": [
    "## II. term-term matrix (或word-word matrix)\n",
    "1. 结构: 每行每列都对应词汇表中的1个word，table中第i行j列的value是：\n",
    "   $$v_{i,j}=\\# word_j\\ in\\, window\\, of\\, word_i $$\n",
    "   - 行中的word是target word，列中的word是context word。table中第i行j列的值是整个语料中，context word出现在target word的context window中的次数。\n",
    "3. intuition： \\\n",
    "① 用word上下文窗口中的其他word信息，也即co-occurrence count来表达word meaning。 \\\n",
    "② <font color=red>如果两个word总是出现在彼此的context window中，对应位置的co-occurrence count value大，说明这两个word关系紧密。</font>\n",
    "4. vector的数值特点：sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4348e274",
   "metadata": {},
   "source": [
    "## III. measuring similarity and relatedness\n",
    "#### 1. inner product\n",
    "- 已知documents的数量有N个，每个word用N维向量表示，两个word v和w之间的相似性可以用他们vector的inner product来衡量：\n",
    "$$<v, w>= {\\textstyle \\sum_{i=1}^{N}}v_iw_i$$\n",
    "当vector来自term-document matrix时，N=|D|；来自term-term matrix时，N=|V|。\n",
    "- <font color=blue>用inner product衡量similarity存在的问题：如果vector中非零元素的个数越多，或者单个维度上的值越大，那么inner product的结果就越大。</font>这意味着：\n",
    "  1. term-document matrix中，常见高频词汇（the, it等）出现的doc多，非零元素多，而且在每个doc中出现的次数也多，因此他与其他word的inner product值偏大。\n",
    "  2. term-term matrix中，高频词汇与更多词汇有co-occurrence，且总的来说与单个context word的co-occurrence次数也多，所以inner product偏大。\n",
    "上面两种情况并不表示较高的similarity或者relatedness，这些高频词汇提供的信息本身也有限。\n",
    "#### 2. cosine\n",
    "- 针对高频词汇带来的similarity度量不准确的问题，可以对inner product做normalization：$$\\frac{v·w}{|v||w|}=\\cos \\theta$$\n",
    "- 含义：此时向量之间的夹角表示两个word的relatedness\n",
    "- **[rk's note]**\n",
    "  - <font color=red>另一种理解角度：高维空间中向量主要分布在超球体表面，因此cosine衡量similarity也是合理的指标。</font>\n",
    "  - normalize之后得到的都是单位化向量，没有长度信息，只有方向信息。从频次counting的意义上来看，以列向量为例，单个word的单位化向量表达的大致信息是它出现的频次分布在各个windows中的比例。\n",
    "    - 如果两次word的列向量接近，说明他们在各个windows中出现的比例接近。\n",
    "    - 但这两个词可能一个是高频词，一个是低频词，它们在所有windows中出现的总次数差异也许很大，但使用cosine衡量similarity时，这个信息不会被用于衡量他们的相似性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5949a170",
   "metadata": {},
   "source": [
    "## IV. TF-IDF和PPMI\n",
    "1. **背景**：简单的co-occurrence count无法很好地度量words之间的relatedness，一个很大的问题还是来自常见高频词。\\\n",
    "① 在term-document matrix中，如果word经常出现在某个doc中，但不经常出现在别的doc中，这个word对于标记该doc会很重要；如果word经常出现在所有doc中，那么它对于标记doc就不重要。\\\n",
    "② 在term-term matrix中，如果word经常出现在特定word的context window中，但不是经常出现在大量的words的context window中，那么前一种word所表达的信息就比后一种更多。\n",
    "2. **solution**：\\\n",
    "① tf-idf weights给matrix中的value加上weights，让信息量大的words的weights大，信息量小的常见高频词的weights小，用weights修正有信息词和无信息高频词的vector value，实现两种情况的区分。\\\n",
    "② ppmi则是直接用term-term matrix重新计算一套衡量relatedness的数值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eaa6ad",
   "metadata": {},
   "source": [
    "#### II.4.1 TF-IDF model\n",
    "1. 方法 \n",
    "$$\\begin{align} \n",
    "  w_{t,d} & = tf_{t, d}*idf_t \\\\\n",
    "\\\\\n",
    "其中， tf_{t, d} & = log_{10}(count(t,d)+1)\\\\\n",
    "idf_t & = log_{10}(\\frac{N}{df_t})\\\\\n",
    " df_t & =  {\\textstyle \\sum_{i=1}^{N}}\\#1\\{word_t\\ in\\ doc_i\\} \n",
    "\\end{align}$$\n",
    "2. 思路 \\\n",
    "⑴ <font color=blue>$tf_{t,d}$是term frequency of word t in document d.</font>\\\n",
    "① 不用raw frequency，因为出现100次的word对doc的重要性不太可能是出现1次的词的100倍，所以用$log_{10}$来降低数量级。\\\n",
    "② 加1是为了避免log0 \\\n",
    "⑵ <font color=blue>$idf_t$是inverse document frequency。</font> \\\n",
    "① 如果一个词出现在很多doc中，那么它的idf就会更低。\\\n",
    "② 由于N是doc的数量，这个值通常都很大，所以也用log来降低数量级\n",
    "3. 应用场景：信息检索中用的多"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8dd741",
   "metadata": {},
   "source": [
    "#### II.4.2 PPMI (positive pointwise mutual information)\n",
    "1. 直觉：衡量两个word之间相关性的最好方式是，先计算假如完全是by chance的话，两个word的co-occurrence的概率，然后看他们实际co-occurrence的概率。如果超过by chance的话，那么他们就相关，超过越多，相关性越强。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1af57",
   "metadata": {},
   "source": [
    "2. **pointwise mutual information,PMI**：两个word实际co-occurrence的概率与假设他们相互独立时，co-occurrence的概率之比。用于衡量两个word之间的相关性。 \\\n",
    "⑴ 定义\n",
    "$$I(x, y)=log_2\\frac{P(x, y)}{P(x)P(y)}\n",
    "\\ \\Rightarrow \\begin{cases}\n",
    " =\\frac{1}{\\alpha } >0 & \\text{ if } 两个词总是同时出现，单个词的概率为\\alpha \\\\\n",
    " =0 & \\text{ if } 两个词独立，没有关联 \\\\\n",
    " <0 & \\text{ if } 两个词很少同时出现 \\\\\n",
    "\\rightarrow -\\infty  & \\text{ if } 两个词几乎从来不同时出现\n",
    "\\end{cases}$$\n",
    "⑵ 问题：PMI的值为负的时候，通常不可靠，除非语料的长度趋于无穷大。 \\\n",
    "假如单个word发生的概率是$10^{-6}$，那么他们独立时$P(x, y)=10^{-12}$。如果co-occurrence的概率比这个概率更低，比如$P(x, y)=10^{-13}$，这就意味着，corpus的长度至少要10万亿以上长度的corpus。\\\n",
    "⑶ solution: 使用Positive PMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe086e9",
   "metadata": {},
   "source": [
    "3. **positive PMI**: \\\n",
    "⑴ 定义\n",
    "$$PPMI(x, y)=max\\left ( log_2\\frac{P(x, y)}{P(x)P(y)}, 0 \\right ) $$\n",
    "此时的取值范围是：\n",
    "$$PPMI(x, y) \\begin{cases}\n",
    " =\\frac{1}{\\alpha } >0 & \\text{ if } 两个词常常同时出现，单个词的概率为\\alpha \\\\\n",
    " =0 & \\text{ if } 两个词独立，或很少同时出现\n",
    "\\end{cases}$$\n",
    "⑵ <font color=green>**如何解决高频词汇信息量低的问题：**</font>\\\n",
    "如果x是高频词汇$P(x)=\\alpha$，y是低频词汇$P(y)=\\beta$，且y出现的时候有一半的时间x会出现，那么$P(x, y)=0.5*\\beta$，此时$PPMI(x,y)=\\frac{1}{2\\alpha}$。假如x是高频无信息次，那么$\\alpha$值相对大，$PPMI(x,y)$的值就小，相对而言就比使用原始count降低了x和y的相关性。 \\\n",
    "⑶ <font color=red>**问题**：biased toward infrequent events.出现频次很低的word倾向于有较高的PPMI value </font> \\\n",
    "· 原因分析：如取值范围的计算公式中，如果α很小，那么1/α就会很大。 \\\n",
    "⑷ **解决方案**: 改变context word的probability计算方式，认为修正结果:\n",
    "$$\\begin{align} \n",
    "  PPMI(x, y)& =max\\left ( log_2\\frac{P(x, y)}{P(x)P_\\alpha (y)}, 0 \\right ) \\\\\n",
    "P_\\alpha (y)& =\\frac{count(y)^\\alpha }{ {\\textstyle \\sum_{y}^{}}count(y)^\\alpha  } \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d7489b",
   "metadata": {},
   "source": [
    "#### II.4.3 注意tf-idf和PPMI的区别\n",
    "tf-idf作为weights与co-occurrence matrix中的value相乘得到新的co-occurrence matrix。可以用cosine of new vector来衡量similarity or relatedness.\n",
    "但是PPMI得到的结果就直接是word vector，不用做为weights来和原来的count matrix相乘"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224nhw",
   "language": "python",
   "name": "cs224nhw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
